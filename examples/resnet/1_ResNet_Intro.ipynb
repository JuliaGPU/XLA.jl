{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [`XLA.jl`](https://github.com/JuliaTPU/XLA.jl): ResNet on TPUs - Introduction\n",
    "\n",
    "In this notebook, we will step through the fundamental infrastructure necessary to load a [ResNet50](https://arxiv.org/abs/1512.03385) model, JIT it for the [TPU](https://en.wikipedia.org/wiki/Tensor_processing_unit), and feed it with some data in order to get classifications out.  Once you are comfortable with this material, you may wish to move on to [simple model training on the TPU](2_ResNet_Training.ipynb), followed by [distributed TPU training](3_ResNet_DistributedTraining.ipynb).\n",
    "\n",
    "## Overview of `XLA.jl` workflow\n",
    "\n",
    "We will define a model in plain Julia using the [`Flux.jl`](https://github.com/FluxML/Flux.jl) framework, that will provide the ResNet 50 model computation.  The model definition is contained within the file [`resnet50.jl`](resnet50.jl), however note that in the near future this will instead be sourced from the Metalhead.jl repository of general computer vision models defined in `Flux.jl`/Julia.\n",
    "\n",
    "We will define a simple set of mappings to convert a standard Julia model to be TPU-runnable.  There are a number of restrictions within the current XLA.jl compiler that must be adhered to for compilation to succeed:\n",
    "\n",
    "* All arrays and scalars must be of type `XRTArray` and must have an element type of `Float32`, including literals.  This unfortunately means that code such as `1./x` must be transformed to `XRTArray(1f0)./x`.  We intend to make this an automatic process in the future, but for the time being we manually define the appropriate helper functions such as `softmax()` that use `XRTArray`'s properly within [`model_utils.jl`](model_utils.jl).\n",
    "\n",
    "* All arrays are immutable, meaning that the definition of some layers such as Batch Normalization in `Flux.jl` must be adapted.  We have created two separate versions of the `BatchNorm` layer in this repository, one meant to be used on the [TPU](tpu_batch_norm.jl) (which we will be using here) and one on the [CPU](cpu_batch_norm.jl), for testing and verification.\n",
    "\n",
    "We will now load in the necessary packages, instantiating an environment local to these notebooks, and construct the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Load package versions that are known to work with TPUs, check that Julia version is a known compatible one\n",
    "if Base.GIT_VERSION_INFO.commit != \"f1dffc5c8b6b7f960b5e30835631b4caf4434b04\"\n",
    "    @warn(\"Only the very latest Julia version on the `kf/tpu3` branch is supported!\")\n",
    "end\n",
    "\n",
    "import Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Initialized ResNet50 model with 25583464 learnable parameters\n"
     ]
    }
   ],
   "source": [
    "# Load in packages and model definition\n",
    "using TensorFlow, XLA, Flux, Printf\n",
    "include(\"resnet50.jl\")\n",
    "include(\"tpu_batch_norm.jl\")\n",
    "\n",
    "model = resnet50();\n",
    "println(\"=> Initialized ResNet50 model with $(sum(prod(size(p)) for p in params(model))) learnable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a `Flux.jl` ResNet50 model.  The next thing we need to do is to alter the model such that it is compileable via `XLA.jl`.  We will do so by defining a set of mapping functions that take advantage of multiple dispatch to recursively walk the model structure, converting normal arrays to `XRTArray`s, coercing scalar values to `Float32`, and converting `BatchNorm` layers to `TPUBatchNorm` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Mapped model to TPU-specific construction\n"
     ]
    }
   ],
   "source": [
    "# Convert scalars to single-element XRTArrays with eltype Float32:\n",
    "map_to_tpu(x::Real) = XRTArray(convert(Float32, x))\n",
    "\n",
    "# Convert arrays to XRTArrays with eltype Float32\n",
    "map_to_tpu(x::AbstractArray) = XRTArray(Float32.(x))\n",
    "\n",
    "# Strip off the TrackedArray coating to get at the data underneath\n",
    "map_to_tpu(x::TrackedArray) = map_to_tpu(Flux.data(x))\n",
    "\n",
    "# Turn Chain objects into ImmutableChain objects which store the computation within their type signature\n",
    "map_to_tpu(x::Chain) = ImmutableChain(tuple(map(map_to_tpu, x.layers)...))\n",
    "\n",
    "# Convert BatchNorm layers into TPUBatchNorm layers, passing all children straight through,\n",
    "# except for the \"active\" child, which is not used by the TPUBatchNorm\n",
    "map_to_tpu(x::BatchNorm) = TPUBatchNorm(map(map_to_tpu, Flux.children(x))[1:end-1]...)\n",
    "\n",
    "# For all other objects, just map the children through `map_to_tpu`.\n",
    "map_to_tpu(x) = Flux.mapchildren(map_to_tpu, x)\n",
    "\n",
    "\n",
    "# Convert our model to the TPU-compatible version\n",
    "tpu_model = map_to_tpu(model)\n",
    "println(\"=> Mapped model to TPU-specific construction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "\n",
    "At this point, we are ready to compile the model.  In order to do so, we must first connect to a TPU or `xrt_server` binary running on a host.  We will connect here to a TPU running on a certain port, and assign the special global variable name `sess` to a `Session` object.  Once we have connected to the TPU, we can use the `@tpu_compile` macro to compile our model down to an executable handle which can then be invoked to actually run the computation upon an `x`.\n",
    "\n",
    "Compilation can take quite a while.  On the GCE machine this notebook was run on, the first compilation took over 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to TPU on 10.240.7.4\n",
      "=> Compiled model in 38.9 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-11 05:06:22.864611: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n"
     ]
    }
   ],
   "source": [
    "tpu_ip = \"10.240.7.4\"\n",
    "println(\"Connecting to TPU on $(tpu_ip)\")\n",
    "\n",
    "# NOTE: If you are connecting to an actual TPU, use `TPUSession`.  If you are\n",
    "# connecting to an `xrt_server`, use `Session()`.\n",
    "sess = TPUSession(\"$(tpu_ip):8470\")\n",
    "#sess = Session(Graph(); target=\"grpc://$(tpu_ip):8470\")\n",
    "\n",
    "# Generate random input tensor; a batch of two images with spatial dimensions 224x224 and 3 color channels.\n",
    "x = randn(Float32, 224, 224, 3, 2)\n",
    "\n",
    "# Compile the model\n",
    "t_start = time()\n",
    "compilation_handle = @tpu_compile tpu_model(XRTArray(x));\n",
    "t_end = time()\n",
    "\n",
    "println(@sprintf(\"=> Compiled model in %.1f seconds\", t_end - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it is compiled, we can run it using `TensorFlow.jl`'s `run()` method.  We must pass first the compilation handle, then a structure containing the weights of the model, then the tensor to be pushed through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×2 Array{Float32,2}:\n",
       " -9.20049e-6   -1.2162e-5  \n",
       " -2.57845e-5   -2.47399e-5 \n",
       "  2.04847e-5    1.98748e-5 \n",
       " -2.54263e-5   -1.99875e-5 \n",
       " -2.60119e-5   -1.95756e-5 \n",
       "  1.82703e-5    1.93357e-5 \n",
       "  3.11616e-6    3.44445e-6 \n",
       " -6.66996e-6   -9.32722e-6 \n",
       " -6.14329e-5   -5.8134e-5  \n",
       "  4.69457e-5    4.64068e-5 \n",
       " -4.44638e-5   -4.12743e-5 \n",
       "  2.96213e-5    2.88296e-5 \n",
       "  3.7362e-6     5.74233e-6 \n",
       "  ⋮                        \n",
       "  4.58536e-6    1.08529e-5 \n",
       "  4.7623e-5     4.91411e-5 \n",
       "  8.99033e-6    1.02057e-5 \n",
       "  0.000103902   0.000102969\n",
       " -2.28912e-6   -2.34087e-6 \n",
       " -8.8403e-5    -8.85922e-5 \n",
       " -6.12964e-5   -5.65298e-5 \n",
       " -1.80922e-5   -1.55818e-5 \n",
       "  1.09936e-5    8.16519e-6 \n",
       " -3.59232e-5   -3.59743e-5 \n",
       " -3.13594e-5   -2.93179e-5 \n",
       " -1.48641e-5   -1.30751e-5 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error in running finalizer: ErrorException(\"type TPUSession has no field ptr\")\n"
     ]
    }
   ],
   "source": [
    "# Run the actual computation\n",
    "y_hat = run(compilation_handle,\n",
    "    # Transfer model weights\n",
    "    XRTRemoteStruct(sess, tpu_model),\n",
    "    # Transfer `x`\n",
    "    XRTArray(sess, x)\n",
    ")\n",
    "\n",
    "# Convert the output (which is an XRTArray) back to a normal array:\n",
    "y_hat = convert(Array, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, you have successfully translated, compiled, and run your first model on the TPU.  Congratulations!  You should feel very proud of yourself.  Next up, we will [learn to do some training](2_ResNet_Training.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "733e5a1d58064defbc9cd3baec9e5cc1",
   "lastKernelId": "93d1a55a-d124-48b1-bc59-c0edb6c54cd7"
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0-DEV",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
