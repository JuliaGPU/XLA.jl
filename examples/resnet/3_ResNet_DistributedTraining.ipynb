{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [`XLA.jl`](https://github.com/JuliaTPU/XLA.jl): ResNet on TPUs - Distributed Training\n",
    "\n",
    "This notebook will show how to train ImageNet distributed across a single TPU device, using all eight cores.  In order to maintain peak efficiency, we will need to make some changes to our data pipeline.  First and foremost, we will be launching eight separate "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "d8c1c5a5e5cc446489cca5de7dc22099",
   "lastKernelId": "78fbf83a-d59c-4c07-9694-6794c3860c7a"
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0-DEV",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
