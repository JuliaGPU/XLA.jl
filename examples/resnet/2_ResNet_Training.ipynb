{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [`XLA.jl`](https://github.com/JuliaTPU/XLA.jl): ResNet on TPUs - Training\n",
    "\n",
    "In this notebook, we will build on [the previous notebook](1_ResNet_Intro.ipynb) introducing the training loop on TPUs for the ResNet 50 computer vision model.  We will use the same model, this time embedding its forward pass within a loop that streams batches of data in through an XLA InFeed and streams training loss out through an XLA OutFeed.  We essentially build an autonomous program which accepts tensors fed in over the network, calculates forward pass, backward pass, updates the model, and then finally returns the fully trained model.  This means that once the TPU code is running, it runs until the model is fully trained, the only interaction required with the TPU is feeding in data.\n",
    "\n",
    "For simplicity and speed, we make use of a preprocessed ImageNet dataset, where each image has been transformed into a `224x224x3` array of UInt8's.  These are transferred to the TPU, where they are unpacked and normalized into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Load package versions that are known to work with TPUs, check that Julia version is a known compatible one\n",
    "if Base.GIT_VERSION_INFO.commit != \"f1dffc5c8b6b7f960b5e30835631b4caf4434b04\"\n",
    "    @warn(\"Only the very latest Julia version on the `kf/tpu3` branch is supported!\")\n",
    "end\n",
    "\n",
    "import Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Initialized ResNet50 model with 25583464 learnable parameters\n",
      "=> Mapped model to TPU-specific construction\n"
     ]
    }
   ],
   "source": [
    "# Load in packages and model definition\n",
    "using TensorFlow, XLA, Flux, Printf\n",
    "include(\"resnet50.jl\")\n",
    "include(\"tpu_batch_norm.jl\")\n",
    "include(\"preprocessing_utils.jl\")\n",
    "include(\"model_utils.jl\")\n",
    "\n",
    "model = resnet50();\n",
    "println(\"=> Initialized ResNet50 model with $(sum(prod(size(p)) for p in params(model))) learnable parameters\")\n",
    "\n",
    "# Convert our model to the TPU-compatible version\n",
    "tpu_model = map_to_tpu(model)\n",
    "println(\"=> Mapped model to TPU-specific construction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "We will define here a training loop that will run as a program on the TPU, taking batches in through an infeed, calculating forward and backward passes, updating weights, and outputting training loss through an outfeed.  We make use of [`Zygote.jl`](https://github.com/FluxML/Zygote.jl) to automatically differentiate the model, generating a function that calculates the backward pass and applying the updates using a custom-built `ADAM` implementation.\n",
    "\n",
    "First, getting data onto the device.  TPUs do not support UInt8 arrays at the time of writing, so we pack a single pixel's (R, G, B) values into a UInt32, and transfer tensors of UInt32's across the wire to the TPU.  See [`preprocessing_utils.jl`](preprocessing_utils.jl) for more on that.  We define a method called `getminibatch_data()` that will read from an infeed, convert the pixel-packed values to `Float32` tensors ready for pushing through the model, and expanding the provided labels into onehot matrices.  Note that the `Val{batch_size}` is because we need this method to be completely statically inferrable (including the size of all tensors).  We pass in `batch_size` as a value type parameter to support compiling models for different batch sizes easily, whereas spatial resolution (`224x224` in this case) is hardcoded as that is much less likely to change, however the same treatment could be given to those values to create a more general infeed function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_minibatch_data (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_minibatch_data(::Val{batch_size}) where {batch_size}\n",
    "    # Construct HloInfeed object that will expect to receive a tuple\n",
    "    # of two arrays, one for `x` and one for `y`.  Note that incorrect sizes\n",
    "    # here will cause...unexpected results, so do your best not to do that.\n",
    "    infeed = XLA.HloInfeed(Tuple{\n",
    "        XRTArray{UInt32, (224*224*batch_size,), 1},\n",
    "        XRTArray{UInt32, (batch_size,), 1},\n",
    "    })\n",
    "    # Read in from the infeed\n",
    "    (x, y), _ = infeed(XLA.HloAfterAll()())\n",
    "    x = reshape(x, (224, 224, batch_size))\n",
    "    \n",
    "    # Do pixel unpacking/channel normalization.\n",
    "    # We feed one-dimensional vectors, so we have to reshape as well.\n",
    "    x = unpack_pixels(x)\n",
    "\n",
    "    # Convert labels to onehot represnetation\n",
    "    y = make_onehot(y)\n",
    "    \n",
    "    # Return our data!\n",
    "    return x, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, optimizer definition.  We hand-craft a simple SGD implementation here; for a more advanced optimizer see the [`ADAM_tpu.jl`](ADAM_tpu.jl) file, used in the next tutorial on distributed TPU training.  `ADAM` is slightly more complex as it must track gradient statistics for each weight in the model, complicating the update step.  In this example, we simply define an `SGD` type for dispatch purposes, then define a recursive update rule that will walk the model weights and gradients, updating as it goes and returning a new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update! (generic function with 4 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct SGD\n",
    "    # Learning rate; the only parameter this optimizer needs to keep track of\n",
    "    η\n",
    "end\n",
    "\n",
    "# Simplest update step in existence.\n",
    "update!(model::AbstractArray, Δ::AbstractArray, η) = model .- Δ .* η\n",
    "\n",
    "# If this leaf node had no updates calculated for it, then skip out early.\n",
    "update!(model, Δ::Nothing, η) = model\n",
    "\n",
    "function update!(model, Δ, η)\n",
    "    # Base condition; if we have reached a leaf node return the inputs unchanged.\n",
    "    # Note that if `model` is an XRTArray, we will hit the override above that actually\n",
    "    # updates the model rather than this generic update!(), same for if Δ is `nothing`.\n",
    "    if nfields(model) == 0\n",
    "        return model\n",
    "    end\n",
    "    \n",
    "    # Recursively pass the fields of this model through the update machinery.  We use\n",
    "    # this strange ntuple() do-block because we cannot perform any kind of mutation\n",
    "    # (such as push!()'ing onto a list) and so we adopt this more functional-style of\n",
    "    # programming.\n",
    "    new_fields = ntuple(Val(nfields(model))) do i\n",
    "        update!(getfield(model, i), getfield(Δ, i), η)\n",
    "    end\n",
    "    \n",
    "    # Return something of the same type as `model`, but with the new fields\n",
    "    if isa(model, Tuple)\n",
    "        return tuple(new_fields...)\n",
    "    else\n",
    "        return typeof(model)(new_fields...)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Main entry point for this optimizer's update steps\n",
    "update!(opt::SGD, model::ImmutableChain, Δ) = update!(model.layers, Δ.layers, opt.η)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the full training loop.  Now that we have the above pieces, this is conceptually very simple.  We will first initialize our optimizer object (not much to do there for SGD, but once we start using ADAM, this will become a little more involved), then we enter the minibatch-pushing loop.  This loop will infeed a new batch of data, push it through the model calculating loss, then backpropagate minimizing that loss in order to calculate a set of updates that should be applied to the model.  We then apply those updates to the model, finally outputting the training loss for this minibatch back to the controlling host.  Finally, once we have exceeded `nbatches` of training data, we return the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_loop (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define our training loop\n",
    "function train_loop(::Val{batch_size}, model, nbatches, η) where {batch_size}\n",
    "    # Initialize optimizer, will allocate space for all necessary statistics within itself\n",
    "    opt = SGD(η)\n",
    "\n",
    "    # Run until nbatches is zero\n",
    "    while nbatches > XRTArray(0)\n",
    "        # Get next minibatch of data\n",
    "        mb_data = get_minibatch_data(Val(batch_size))\n",
    "\n",
    "        # Let block to fend off the inference demons\n",
    "        loss, back = let x = mb_data[1], y = mb_data[2]\n",
    "            # Calculate forward pass to get loss, and compile backwards pass\n",
    "            # to get the updates to our model weights.\n",
    "            Zygote._forward(\n",
    "                Zygote.Context{Nothing}(nothing),\n",
    "                model -> logitcrossentropy(model(x), y),\n",
    "                model,\n",
    "            )\n",
    "        end\n",
    "\n",
    "        # Evaluate the backwards pass.  Zygote automatically calculates\n",
    "        # sensitivities upon `x` and `y`; we discard those via the tail()\n",
    "        Δ_model = Zygote.tailmemaybe(back(1f0))[1]\n",
    "\n",
    "        # Update parameters via our optimizer\n",
    "        model = update!(opt, model, Δ_model)\n",
    "\n",
    "        # Outfeed the loss\n",
    "        #loss = reshape(loss, (1,))\n",
    "        XLA.HloOutfeed()((loss,), XLA.HloAfterAll()())\n",
    "\n",
    "        # Count down the batches\n",
    "        nbatches -= XRTArray(1)\n",
    "    end\n",
    "    \n",
    "    # At the end of all things, return the trained model\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "debug2 (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This works\n",
    "function debug2(::Val{batch_size}, model) where {batch_size}\n",
    "    opt = SGD(0.001f0)\n",
    "\n",
    "    mb_data = get_minibatch_data(Val(batch_size))\n",
    "    \n",
    "    loss, back = let x = mb_data[1], y = mb_data[2]\n",
    "        Zygote._forward(\n",
    "            Zygote.Context{Nothing}(nothing),\n",
    "            model -> logitcrossentropy(model(x), y),\n",
    "            model,\n",
    "        )\n",
    "    end\n",
    "\n",
    "    Δ_model = Zygote.tailmemaybe(back(1f0))[1]\n",
    "    model = update!(opt, model, Δ_model)\n",
    "    \n",
    "    XLA.HloOutfeed()((loss,), XLA.HloAfterAll()())\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training loop\n",
    "\n",
    "Now that we've got all that code written up, let's actually run the training loop.  First, we compile it.  Again, this can take a _very_ long time (on the GCE instance this notebook was run on, this took over 60 seconds), so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to TPU on 10.240.7.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-28 07:16:59.088129: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n"
     ]
    },
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 23312-element Array{Int64,1} at index [0]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 23312-element Array{Int64,1} at index [0]",
      "",
      "Stacktrace:",
      " [1] getindex at ./array.jl:731 [inlined]",
      " [2] NCA(::Core.Compiler.DomTree, ::Array{Int64,1}) at /home/sabae/.julia/dev/XLA/src/outlining.jl:173",
      " [3] analyze_regions(::Core.Compiler.DomTree, ::Core.Compiler.CFG) at /home/sabae/.julia/dev/XLA/src/outlining.jl:215",
      " [4] outline_control_flow!(::Core.Compiler.IRCode, ::Core.Compiler.OptimizationState) at /home/sabae/.julia/dev/XLA/src/outlining.jl:461",
      " [5] _compile_to_xla!(::Array{XLA.xla.HloComputationProto,1}, ::XLA.xla.HloComputationProto, ::Core.Compiler.IRCode, ::Core.Compiler.OptimizationState) at /home/sabae/.julia/dev/XLA/src/compiler.jl:47",
      " [6] #compile_to_xla#184(::Nothing, ::Function, ::Core.Compiler.IRCode, ::Core.Compiler.OptimizationState) at /home/sabae/.julia/dev/XLA/src/compiler.jl:324",
      " [7] (::getfield(XLA, Symbol(\"#kw##compile_to_xla\")))(::NamedTuple{(:replica_device_coords,),Tuple{Nothing}}, ::typeof(XLA.compile_to_xla), ::Core.Compiler.IRCode, ::Core.Compiler.OptimizationState) at ./none:0",
      " [8] top-level scope at /home/sabae/.julia/dev/XLA/src/compiler_interface.jl:109",
      " [9] top-level scope at In[6]:19"
     ]
    }
   ],
   "source": [
    "tpu_ip = \"10.240.7.4\"\n",
    "println(\"Connecting to TPU on $(tpu_ip)\")\n",
    "\n",
    "# NOTE: If you are connecting to an actual TPU, use `TPUSession`.  If you are\n",
    "# connecting to an `xrt_server`, use `Session()`.\n",
    "sess = TPUSession(\"$(tpu_ip):8470\")\n",
    "#sess = Session(Graph(); target=\"grpc://$(tpu_ip):8470\")\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_batches = 1000\n",
    "η = 0.001\n",
    "\n",
    "x = randn(Float32, 224, 224, 3, 1)\n",
    "y = rand(Float32, 1000, 1)\n",
    "\n",
    "# Compile the model\n",
    "t_start = time()\n",
    "#compilation_handle = @tpu_compile debug2(Val(batch_size), tpu_model)\n",
    "compilation_handle = @tpu_compile train_loop(Val(batch_size), tpu_model, XRTArray(num_batches), XRTArray(η));\n",
    "t_end = time()\n",
    "\n",
    "println(@sprintf(\"=> Compiled training loop in %.1f seconds\", t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = run(compilation_handle, XRTRemoteStruct(sess, tpu_model))\n",
    "ret = convert(typeof(ret).parameters[1], ret);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Base.IRShow.show_ir(stdout, XLA.code_typed_xla(Tuple{typeof(train_loop), typeof(Val(batch_size)), typeof(tpu_model), typeof(XRTArray(num_batches)), typeof(XRTArray(η))})[1]; verbose_linetable=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XLA.explain_suboptimal_inference(Tuple{typeof(debug2), typeof(Val(batch_size)), typeof(tpu_model)})\n",
    "XLA.explain_suboptimal_inference(Tuple{typeof(train_loop), typeof(Val(batch_size)), typeof(tpu_model), typeof(XRTArray(num_batches)), typeof(XRTArray(η))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "3e8dc8f99a854ee68ba064ebf565cc87",
   "lastKernelId": "43cc72a0-b5e1-4946-b37a-1fef52fed044"
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0-DEV",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
