{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [`XLA.jl`](https://github.com/JuliaTPU/XLA.jl): ResNet on TPUs - Training\n",
    "\n",
    "In this notebook, we will build on [the previous notebook](1_ResNet_Intro.ipynb) by introducing the training loop on TPUs for the ResNet 50 computer vision model.  We will use the same model, this time embedding its forward pass within a loop that streams batches of data in through an XLA InFeed and streams training loss out through an XLA OutFeed.  We essentially build an autonomous program which accepts tensors fed in over the network, calculates forward pass, backward pass, updates the model, and then finally returns the fully trained model.  This means that once the TPU code is running, it runs until the model is fully trained, the only interaction required with the TPU is feeding in data.\n",
    "\n",
    "For simplicity and speed, we make use of a preprocessed ImageNet dataset, where each image has been transformed into a `224x224x3` array of UInt8's and each label has been transformed into a `UInt16`, which are stored on disk.  The image preprocessing steps can be found in [this notebook](PreprocessedImagenet.ipynb), for the terminally curious.  These are transferred to the TPU, where they are then unpacked and normalized into `Float32` tensors, ready to be run through the Flux model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Load package versions that are known to work with TPUs, check that Julia version is a known compatible one\n",
    "if Base.GIT_VERSION_INFO.commit != \"f1dffc5c8b6b7f960b5e30835631b4caf4434b04\"\n",
    "    @warn(\"Only the very latest Julia version on the `kf/tpu3` branch is supported!\")\n",
    "end\n",
    "\n",
    "import Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/sabae/.julia/compiled/v1.1/XLA/bZBiw.ji for XLA [1ae4bca4-de81-11e8-0eca-6d3e4e7c4181]\n",
      "└ @ Base loading.jl:1184\n",
      "WARNING: could not import xla.Shape into XLA\n",
      "WARNING: could not import NNlib.cudata into Tracker\n",
      "WARNING: Method definition softmax(XLA.XRTArray{T, Dims, N} where N where Dims where T) in module XLA at /home/sabae/.julia/dev/XLA/src/linalg.jl:403 overwritten at /home/sabae/.julia/dev/XLA/src/utils.jl:115.\n",
      "WARNING: Method definition ∇softmax(Any, XLA.XRTArray{T, Dims, N} where N where Dims where T) in module XLA at /home/sabae/.julia/dev/XLA/src/linalg.jl:407 overwritten at /home/sabae/.julia/dev/XLA/src/utils.jl:120.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Initialized ResNet50 model with 25583464 learnable parameters\n",
      "=> Mapped model to TPU-specific construction\n"
     ]
    }
   ],
   "source": [
    "# Load in packages and model definition\n",
    "using TensorFlow, XLA, Flux, Zygote, Printf\n",
    "include(\"resnet50.jl\")\n",
    "include(\"preprocessing_utils.jl\")\n",
    "include(\"model_utils.jl\")\n",
    "\n",
    "model = resnet50();\n",
    "println(\"=> Initialized ResNet50 model with $(sum(prod(size(p)) for p in params(model))) learnable parameters\")\n",
    "\n",
    "# Convert our model to the TPU-compatible version\n",
    "tpu_model = map_to_tpu(model)\n",
    "println(\"=> Mapped model to TPU-specific construction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "We will define here a training loop that will run as a program on the TPU, taking batches in through an infeed, calculating forward and backward passes, updating weights, and outputting training loss through an outfeed.  We make use of [`Zygote.jl`](https://github.com/FluxML/Zygote.jl) to automatically differentiate the model, generating a function that calculates the backward pass and applying the updates using a custom-built `SGD` implementation.\n",
    "\n",
    "First, getting data onto the device.  TPUs do not support UInt8 arrays at the time of writing, so we pack a single pixel's (R, G, B) values into a UInt32, and transfer tensors of UInt32's across the wire to the TPU.  See [`preprocessing_utils.jl`](preprocessing_utils.jl) for more on that.  We define a method called `getminibatch_data()` that will read from an infeed, convert the pixel-packed values to `Float32` tensors ready for pushing through the model, and expanding the provided labels into onehot matrices.  Note that the `Val{batch_size}` is because we need this method to be completely statically inferrable (including the size of all tensors).  We pass in `batch_size` as a value type parameter to support compiling models for different batch sizes easily, whereas spatial resolution (`224x224` in this case) is hardcoded as that is much less likely to change, however the same treatment could be given to those values to create a more general infeed function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_minibatch_data (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_minibatch_data(::Val{batch_size}) where {batch_size}\n",
    "    # Construct HloInfeed object that will expect to receive a tuple\n",
    "    # of two arrays, one for `x` and one for `y`.  Note that incorrect sizes\n",
    "    # here will cause...unexpected results, so do your best not to do that.\n",
    "    # We feed data in as 1-dimensional UInt32 arrays, then reshape them.\n",
    "    infeed = XLA.HloInfeed(Tuple{\n",
    "        XRTArray{UInt32, (224*224*batch_size,), 1},\n",
    "        XRTArray{UInt32, (batch_size,), 1},\n",
    "    })\n",
    "\n",
    "    # Read in from the infeed\n",
    "    (x, y), _ = infeed(XLA.HloAfterAll()())\n",
    "    x = reshape(x, (224, 224, batch_size))\n",
    "    \n",
    "    # Do pixel unpacking/channel normalization.\n",
    "    x = unpack_pixels(x)\n",
    "\n",
    "    # Convert labels to (dense) onehot representation\n",
    "    y = make_onehot(y)\n",
    "    #y = convert(XRTArray{Float32}, Flux.OneHotMatrix(1000, convert(XRTArray{Int64}, y)))\n",
    "    \n",
    "    # Return our data!\n",
    "    return x, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, optimizer definition.  We hand-craft a simple SGD implementation here; for a more advanced optimizer see the [`ADAM_tpu.jl`](ADAM_tpu.jl) file, used in the next tutorial on distributed TPU training.  `ADAM` is slightly more complex as it must track gradient statistics for each weight in the model, complicating the update step.  In this example, we simply define an `SGD` type for dispatch purposes, then define a recursive update rule that will walk the model weights and gradients, updating as it goes and returning a new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update! (generic function with 4 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct SGD\n",
    "    # Learning rate; the only data this optimizer needs to bundle with itself\n",
    "    η::XRTArray{Float32,(),0}\n",
    "end\n",
    "\n",
    "# Simplest update step in existence.\n",
    "update!(model::XRTArray, Δ::XRTArray, η) = model - (Δ .* η)\n",
    "\n",
    "# If this leaf node had no updates calculated for it, then skip out early.\n",
    "update!(model, Δ::Nothing, η) = model\n",
    "\n",
    "function update!(model, Δ, η)\n",
    "    # Base condition; if we have reached a leaf node return the inputs unchanged.\n",
    "    # Note that if `model` is an XRTArray, we will hit the override above that actually\n",
    "    # updates the model rather than this generic update!(), same for if Δ is `nothing`.\n",
    "    if nfields(model) == 0\n",
    "        return model\n",
    "    end\n",
    "    \n",
    "    # Recursively pass the fields of this model through the update machinery.  We use\n",
    "    # this strange ntuple() do-block because we cannot perform any kind of mutation\n",
    "    # (such as push!()'ing onto a list) and so we adopt this more functional-style of\n",
    "    # programming.\n",
    "    new_fields = ntuple(Val(nfields(model))) do i\n",
    "        return update!(getfield(model, i), getfield(Δ, i), η)\n",
    "    end\n",
    "    \n",
    "    # Return something of the same type as `model`, but with the new fields\n",
    "    if isa(model, Tuple)\n",
    "        return new_fields\n",
    "    else\n",
    "        return typeof(model)(new_fields...)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Main entry point for this optimizer's update steps\n",
    "update!(opt::SGD, model, Δ) = update!(model, Δ, opt.η)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the full training loop.  Now that we have the above pieces, this is conceptually very simple.  We will first initialize our optimizer object (not much to do there for SGD, but once we start using ADAM, this will become a little more involved), then we enter the minibatch-processing loop.  This loop will infeed a new batch of data, push it through the model calculating loss, then backpropagate, minimizing that loss in order to calculate a set of updates that should be applied to the model.  We then apply those updates to the model, finally outputting the training loss for this minibatch back to the controlling host.  Finally, once we have exceeded `nbatches` of training data, we return the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_loop (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define our training loop\n",
    "function train_loop(::Val{batch_size}, model, nbatches, η) where {batch_size}\n",
    "    # Initialize optimizer, will allocate space for all necessary statistics within itself\n",
    "    opt = SGD(η)\n",
    "\n",
    "    # Run until nbatches is zero\n",
    "    while nbatches > XRTArray(0)\n",
    "        # Get next minibatch of data\n",
    "        mb_data = get_minibatch_data(Val(batch_size))\n",
    "\n",
    "        # Let block to fend off the inference demons\n",
    "        loss, back = let x = mb_data[1], y = mb_data[2]\n",
    "            # Calculate forward pass to get loss, and compile backwards pass\n",
    "            # to get the updates to our model weights.\n",
    "            Zygote._forward(\n",
    "                Zygote.Context{Nothing}(nothing),\n",
    "                model -> logitcrossentropy(model(x), y),\n",
    "                model,\n",
    "            )\n",
    "        end\n",
    "\n",
    "        # Evaluate the backwards pass.  Zygote automatically calculates\n",
    "        # sensitivities upon `x` and `y`; we discard those via the tail()\n",
    "        Δ_model = Zygote.tailmemaybe(back(1f0))[1]\n",
    "\n",
    "        # Update parameters via our optimizer\n",
    "        model = update!(opt, model, Δ_model)\n",
    "\n",
    "        # Outfeed the loss\n",
    "        loss = reshape(loss, (1,))\n",
    "        XLA.HloOutfeed()((loss,), XLA.HloAfterAll()())\n",
    "\n",
    "        # Count down the batches\n",
    "        nbatches -= XRTArray(1)\n",
    "    end\n",
    "    \n",
    "    # At the end of all things, return the trained model\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training loop\n",
    "\n",
    "Now that we've got all that code written up, let's actually run the training loop.  First, we compile it.  Again, this can take a _very_ long time (on the GCE instance this notebook was run on, this took over 60 seconds), so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to TPU on 10.240.17.5\n",
      "=> Compiled training loop in 217.9 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-17 00:41:59.311681: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n"
     ]
    }
   ],
   "source": [
    "#tpu_ip = \"10.240.25.2\"\n",
    "tpu_ip = \"10.240.17.5\"\n",
    "println(\"Connecting to TPU on $(tpu_ip)\")\n",
    "\n",
    "# NOTE: If you are connecting to an actual TPU, use `TPUSession`.  If you are\n",
    "# connecting to an `xrt_server`, use `Session()`.\n",
    "sess = TPUSession(\"$(tpu_ip):8470\")\n",
    "#sess = Session(Graph(); target=\"grpc://$(tpu_ip):8470\")\n",
    "\n",
    "# Train in batch sizes of 128, for 10000 batches with a learning rate of 1e-4\n",
    "batch_size = 128\n",
    "num_batches = 10000\n",
    "η = 0.0001f0\n",
    "\n",
    "# We need to work around a TensorFlow bug which doesn't choose the default infeed/outfeed\n",
    "# device placement properly.  We do so by explicitly placing all operations on the first TPU core\n",
    "tpu_device = first(all_tpu_devices(sess))\n",
    "\n",
    "# Compile the model\n",
    "t_start = time()\n",
    "compilation_handle = @tpu_compile devices=[tpu_device] train_loop(Val(batch_size), tpu_model, XRTArray(num_batches), XRTArray(η));\n",
    "t_end = time()\n",
    "\n",
    "println(@sprintf(\"=> Compiled training loop in %.1f seconds\", t_end - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop is an asynchronous program that will block on the TPU waiting for input from us, so we will launch it on a separate Julia coroutine by using the `async=true` kwarg to `run()`, then after launching it start sending data to the TPU through the infeed and reading output back from the outfeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error in running finalizer: ErrorException(\"type TPUSession has no field ptr\")\n",
      "error in running finalizer: ErrorException(\"type TPUSession has no field ptr\")\n",
      "error in running finalizer: ErrorException(\"type TPUSession has no field ptr\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Task (runnable) @0x00007f497152c010"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explicitly place each argument of our compilation onto the first tpu_device\n",
    "compilation_args = with_device(XLA.tf_tpu_device(sess, tpu_device)) do\n",
    "    return (\n",
    "        XRTRemoteStruct(sess, tpu_model),\n",
    "        XRTArray(sess, num_batches),\n",
    "        XRTArray(sess, η)\n",
    "    )\n",
    "end\n",
    "\n",
    "# Launch a task to run our training loop\n",
    "loop_task = @async XLA.run(sess,\n",
    "    # Again, we must explicitly schedule this to run on the first tpu_device.\n",
    "    XLA.make_xrt_execute_on(\n",
    "        tpu_device,\n",
    "        compilation_handle,\n",
    "        compilation_args...\n",
    "    );\n",
    "    async=true,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that the loop task is actually running (if there is an error on the TPU, it will fail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (runnable) @0x00007f497152c010"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, there is a program running on the TPU that is blocked on input, and so the next step is to provide that input.\n",
    "\n",
    "## Feeding the training loop\n",
    "\n",
    "We will load in data from our preprocessed training set, saved as raw tensors on disk that we can directly memmap in.  Because the data rates that we want to feed our network with often exceed the rate that we can read in from disk, we recommend paying for a large enough instance to fit the entire dataset into RAM, and using a tool such as [`vmtouch`](https://github.com/hoytech/vmtouch) to preload it.  It's important to shuffle your data, so we build a permutation vector to read in with a randomized ordering across images, which works well in conjunction with the aforementioned `vmtouch` to preload your dataset into RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Mmap, Random, Statistics\n",
    "\n",
    "# memmap our dataset of 1.2M images in as giant, raw tensors.\n",
    "num_images = 1281167\n",
    "Xs = Mmap.mmap(\"/home/sabae/ILSVRC/224/X_train.raw\", Array{UInt8,4}, (224, 224, 3, num_images))\n",
    "Ys = Mmap.mmap(\"/home/sabae/ILSVRC/224/Y_train.raw\", Array{UInt16,1}, (num_images,))\n",
    "permutation = Random.shuffle(1:num_images);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next convert slices of data into the one-dimensional `UInt32` arrays that our model expects, and we send them over the wire with `XLA.infeed_and_outfeed()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] loss: 7.71468 avg loss: 7.631979465484619\n",
      "[20] loss: 7.5717015 avg loss: 7.659825539588928\n",
      "[30] loss: 7.602431 avg loss: 7.647730493545533\n",
      "[40] loss: 7.499025 avg loss: 7.6334481716156\n",
      "[50] loss: 7.668681 avg loss: 7.624117631912231\n",
      "[60] loss: 7.384497 avg loss: 7.619833039302452\n",
      "[70] loss: 7.6246686 avg loss: 7.603289398492551\n",
      "[80] loss: 7.5632586 avg loss: 7.596195286395503\n",
      "[90] loss: 7.4807167 avg loss: 7.5845983916637945\n",
      "[100] loss: 7.598644 avg loss: 7.580838511971867\n",
      "[110] loss: 7.6822515 avg loss: 7.565871238708496\n",
      "[120] loss: 7.355796 avg loss: 7.541649687523935\n",
      "[130] loss: 7.432856 avg loss: 7.519176380307067\n",
      "[140] loss: 7.439713 avg loss: 7.498325768639059\n",
      "[150] loss: 7.3923154 avg loss: 7.489733415491441\n",
      "[160] loss: 7.312935 avg loss: 7.469253586787803\n",
      "[170] loss: 7.562125 avg loss: 7.449994872598087\n",
      "[180] loss: 7.518091 avg loss: 7.434520581189324\n",
      "[190] loss: 7.4731655 avg loss: 7.442003502565272\n",
      "[200] loss: 7.293027 avg loss: 7.419354766022925\n",
      "[210] loss: 7.2296853 avg loss: 7.399402590358958\n",
      "[220] loss: 7.457613 avg loss: 7.40663454579372\n",
      "[230] loss: 7.4458156 avg loss: 7.40659901675056\n",
      "[240] loss: 7.433418 avg loss: 7.376331348045199\n",
      "[250] loss: 7.3762755 avg loss: 7.367602843864291\n",
      "[260] loss: 7.250561 avg loss: 7.367851173176485\n",
      "[270] loss: 7.1695676 avg loss: 7.355272358539057\n",
      "[280] loss: 7.4839287 avg loss: 7.341917402604047\n",
      "[290] loss: 7.4285483 avg loss: 7.33786584816727\n",
      "[300] loss: 7.2278075 avg loss: 7.322499359355254\n",
      "[310] loss: 7.214977 avg loss: 7.307306411219578\n",
      "[320] loss: 7.319763 avg loss: 7.296942177940817\n",
      "[330] loss: 7.308282 avg loss: 7.298254115908754\n",
      "[340] loss: 7.1595984 avg loss: 7.286219634261786\n",
      "[350] loss: 7.2045546 avg loss: 7.286939611621931\n",
      "[360] loss: 7.3536882 avg loss: 7.290313458910175\n",
      "[370] loss: 7.230285 avg loss: 7.287390353632908\n",
      "[380] loss: 7.4878197 avg loss: 7.278928392073688\n",
      "[390] loss: 7.2234135 avg loss: 7.273515402102003\n",
      "[400] loss: 7.2876153 avg loss: 7.255737631928687\n",
      "[410] loss: 7.4707265 avg loss: 7.253940675772872\n",
      "[420] loss: 7.1228585 avg loss: 7.250547427757113\n",
      "[430] loss: 7.4536505 avg loss: 7.24978837779924\n",
      "[440] loss: 7.1292586 avg loss: 7.2397554247987035\n",
      "[450] loss: 7.142587 avg loss: 7.251000049067478\n",
      "[460] loss: 7.1714864 avg loss: 7.235391242831361\n",
      "[470] loss: 7.206891 avg loss: 7.223097146726122\n",
      "[480] loss: 7.1844716 avg loss: 7.218990550321691\n",
      "[490] loss: 7.296802 avg loss: 7.211395263671875\n",
      "[500] loss: 7.064185 avg loss: 7.191252091351678\n",
      "[510] loss: 7.2790833 avg loss: 7.195282954795688\n",
      "[520] loss: 7.365542 avg loss: 7.1912283616907455\n",
      "[530] loss: 7.23261 avg loss: 7.19755568223841\n",
      "[540] loss: 7.0447516 avg loss: 7.197177297928754\n",
      "[550] loss: 7.414478 avg loss: 7.199774929121429\n",
      "[560] loss: 7.1884456 avg loss: 7.196687156078863\n",
      "[570] loss: 7.3237777 avg loss: 7.195308114968094\n",
      "[580] loss: 7.5287523 avg loss: 7.189151735866771\n",
      "[590] loss: 7.2514424 avg loss: 7.184005400713752\n",
      "[600] loss: 7.1882367 avg loss: 7.195388737846823\n",
      "[610] loss: 7.1410823 avg loss: 7.191773536158543\n",
      "[620] loss: 7.2424293 avg loss: 7.188835835924335\n",
      "[630] loss: 7.171338 avg loss: 7.190152598362343\n",
      "[640] loss: 7.068453 avg loss: 7.18145362068625\n",
      "[650] loss: 7.1920114 avg loss: 7.169392006070006\n",
      "[660] loss: 7.237299 avg loss: 7.1628429468940285\n",
      "[670] loss: 7.2617774 avg loss: 7.169784891839121\n",
      "[680] loss: 7.039838 avg loss: 7.16051524293189\n",
      "[690] loss: 7.083686 avg loss: 7.146145044588575\n",
      "[700] loss: 7.1868987 avg loss: 7.161150492873847\n",
      "[710] loss: 7.1610274 avg loss: 7.151359128017051\n",
      "[720] loss: 7.2071424 avg loss: 7.140993249182608\n",
      "[730] loss: 7.2884617 avg loss: 7.138132095336914\n",
      "[740] loss: 7.3464804 avg loss: 7.143373704424091\n",
      "[750] loss: 7.0191236 avg loss: 7.128098543952493\n",
      "[760] loss: 7.2845106 avg loss: 7.127519149406283\n",
      "[770] loss: 7.292346 avg loss: 7.136642830044615\n",
      "[780] loss: 7.2930875 avg loss: 7.141014407662785\n",
      "[790] loss: 7.0507426 avg loss: 7.127958681069169\n",
      "[800] loss: 7.198399 avg loss: 7.115914241940367\n",
      "[810] loss: 7.0629573 avg loss: 7.1299622573104555\n",
      "[820] loss: 7.1162467 avg loss: 7.130241496890199\n",
      "[830] loss: 7.260041 avg loss: 7.130284935820336\n",
      "[840] loss: 7.153763 avg loss: 7.149610061271518\n",
      "[850] loss: 7.244175 avg loss: 7.153105754478305\n",
      "[860] loss: 7.1273518 avg loss: 7.149086110732135\n",
      "[870] loss: 7.199724 avg loss: 7.157946212618959\n",
      "[880] loss: 7.1871724 avg loss: 7.160250551560345\n",
      "[890] loss: 7.077498 avg loss: 7.15470401913512\n",
      "[900] loss: 6.9977612 avg loss: 7.149883111317952\n",
      "[910] loss: 7.089616 avg loss: 7.147171441246481\n",
      "[920] loss: 7.270633 avg loss: 7.132725051805084\n",
      "[930] loss: 7.0252733 avg loss: 7.1253846205917055\n",
      "[940] loss: 7.0494866 avg loss: 7.119271091386383\n",
      "[950] loss: 7.134223 avg loss: 7.124384487376494\n",
      "[960] loss: 7.0210657 avg loss: 7.142932246713078\n",
      "[970] loss: 7.368587 avg loss: 7.150447808059991\n",
      "[980] loss: 6.9015603 avg loss: 7.132088651844099\n",
      "[990] loss: 7.0589542 avg loss: 7.1298237127416275\n",
      "[1000] loss: 7.1189175 avg loss: 7.130697661755132\n",
      "[1010] loss: 7.0294466 avg loss: 7.108139729967304\n",
      "[1020] loss: 7.041022 avg loss: 7.108410442576689\n",
      "[1030] loss: 7.025359 avg loss: 7.108710681690889\n",
      "[1040] loss: 7.021379 avg loss: 7.103145197326062\n",
      "[1050] loss: 7.341499 avg loss: 7.103256178837197\n",
      "[1060] loss: 7.1557875 avg loss: 7.106343194550159\n",
      "[1070] loss: 7.076838 avg loss: 7.103717616960114\n",
      "[1080] loss: 7.0094132 avg loss: 7.100416510712867\n",
      "[1090] loss: 7.1978493 avg loss: 7.09797555325078\n",
      "[1100] loss: 7.01745 avg loss: 7.0925824875925105\n",
      "[1110] loss: 6.9319344 avg loss: 7.078685956842759\n",
      "[1120] loss: 7.1924543 avg loss: 7.085132318384507\n",
      "[1130] loss: 6.795413 avg loss: 7.089382620418773\n",
      "[1140] loss: 7.030062 avg loss: 7.101564865486295\n",
      "[1150] loss: 7.1206 avg loss: 7.119247660917394\n",
      "[1160] loss: 7.0088954 avg loss: 7.113359993579341\n",
      "[1170] loss: 6.9579697 avg loss: 7.108275768803615\n",
      "[1180] loss: 7.113838 avg loss: 7.101354916890462\n",
      "[1190] loss: 7.052267 avg loss: 7.096632228178136\n",
      "[1200] loss: 7.0154195 avg loss: 7.087056795756022\n",
      "[1210] loss: 7.0136456 avg loss: 7.081434175079944\n",
      "[1220] loss: 7.298013 avg loss: 7.084946099449606\n",
      "[1230] loss: 7.012956 avg loss: 7.08532813951081\n",
      "[1240] loss: 7.064739 avg loss: 7.080799663768095\n",
      "[1250] loss: 7.104466 avg loss: 7.0718388276941635\n",
      "[1260] loss: 6.916303 avg loss: 7.078090377882416\n",
      "[1270] loss: 7.0024 avg loss: 7.069965764588001\n",
      "[1280] loss: 6.9762034 avg loss: 7.053639804615694\n",
      "[1290] loss: 7.3107986 avg loss: 7.0555409543654495\n",
      "[1300] loss: 7.2008176 avg loss: 7.070101092843449\n",
      "[1310] loss: 7.175105 avg loss: 7.063023183860031\n",
      "[1320] loss: 6.987926 avg loss: 7.071165533626781\n",
      "[1330] loss: 6.9318957 avg loss: 7.071975287269144\n",
      "[1340] loss: 7.107039 avg loss: 7.0795214690414126\n",
      "[1350] loss: 6.992542 avg loss: 7.068487905988507\n",
      "[1360] loss: 7.1783605 avg loss: 7.05940830006319\n",
      "[1370] loss: 6.9890313 avg loss: 7.047344544354607\n",
      "[1380] loss: 6.9881697 avg loss: 7.037043992210837\n",
      "[1390] loss: 6.9695864 avg loss: 7.033167287415149\n",
      "[1400] loss: 7.1663475 avg loss: 7.018806831509459\n",
      "[1410] loss: 7.019977 avg loss: 7.031343787324195\n",
      "[1420] loss: 6.9542537 avg loss: 7.027005345213647\n",
      "[1430] loss: 7.089127 avg loss: 7.036065765455658\n",
      "[1440] loss: 6.983603 avg loss: 7.035615294587378\n",
      "[1450] loss: 7.009852 avg loss: 7.034757371042289\n",
      "[1460] loss: 6.9040427 avg loss: 7.0314917470894605\n",
      "[1470] loss: 7.133263 avg loss: 7.047745835547354\n",
      "[1480] loss: 7.0520186 avg loss: 7.063107986076205\n",
      "[1490] loss: 7.152162 avg loss: 7.068291355581844\n",
      "[1500] loss: 6.972306 avg loss: 7.080029179068172\n",
      "[1510] loss: 7.049787 avg loss: 7.083351546642827\n",
      "[1520] loss: 6.898196 avg loss: 7.073717827890434\n",
      "[1530] loss: 7.281358 avg loss: 7.056698845882042\n",
      "[1540] loss: 7.0151033 avg loss: 7.060884213915058\n",
      "[1550] loss: 7.0738683 avg loss: 7.059296589271695\n",
      "[1560] loss: 7.1386213 avg loss: 7.042824043947108\n",
      "[1570] loss: 6.9596353 avg loss: 7.033517407435997\n",
      "[1580] loss: 6.9277563 avg loss: 7.032745295879888\n",
      "[1590] loss: 6.9470367 avg loss: 7.0111499954672425\n",
      "[1600] loss: 7.1547704 avg loss: 7.007001830082314\n",
      "[1610] loss: 7.024594 avg loss: 7.018178771523869\n",
      "[1620] loss: 7.1315265 avg loss: 7.026755566690483\n",
      "[1630] loss: 7.0115614 avg loss: 7.043292653326895\n",
      "[1640] loss: 7.1594086 avg loss: 7.052665373858283\n",
      "[1650] loss: 7.0757127 avg loss: 7.062807008331897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1660] loss: 7.0714025 avg loss: 7.064877631617527\n",
      "[1670] loss: 7.1298003 avg loss: 7.064521013521681\n",
      "[1680] loss: 7.207738 avg loss: 7.052081295088226\n",
      "[1690] loss: 6.8853197 avg loss: 7.048302977692847\n",
      "[1700] loss: 6.9318714 avg loss: 7.0472016895518585\n",
      "[1710] loss: 6.9775953 avg loss: 7.045128906474394\n",
      "[1720] loss: 6.9703145 avg loss: 7.060499275431914\n",
      "[1730] loss: 7.130493 avg loss: 7.06027972464468\n",
      "[1740] loss: 7.0221286 avg loss: 7.054997275857365\n",
      "[1750] loss: 6.9395847 avg loss: 7.036116319544175\n",
      "[1760] loss: 7.259564 avg loss: 7.044273638257794\n",
      "[1770] loss: 7.1690793 avg loss: 7.026100111942665\n",
      "[1780] loss: 7.1106043 avg loss: 7.040051441566617\n",
      "[1790] loss: 7.0109563 avg loss: 7.045516556384516\n",
      "[1800] loss: 7.147522 avg loss: 7.065769335802863\n",
      "[1810] loss: 7.125301 avg loss: 7.058101195915072\n",
      "[1820] loss: 7.051183 avg loss: 7.056617942510867\n",
      "[1830] loss: 7.0714617 avg loss: 7.0446058815600825\n",
      "[1840] loss: 6.9985833 avg loss: 7.043968509225285\n",
      "[1850] loss: 6.913909 avg loss: 7.026054288826737\n",
      "[1860] loss: 7.065777 avg loss: 7.028324145896762\n",
      "[1870] loss: 7.0337186 avg loss: 7.019472561630548\n",
      "[1880] loss: 7.0763283 avg loss: 7.0207413972592825\n",
      "[1890] loss: 6.9925275 avg loss: 7.015256909763112\n",
      "[1900] loss: 7.240331 avg loss: 7.0343070404202335\n",
      "[1910] loss: 6.968431 avg loss: 7.037685815025778\n",
      "[1920] loss: 6.989656 avg loss: 7.027291185715619\n",
      "[1930] loss: 7.0240393 avg loss: 7.024788547964657\n",
      "[1940] loss: 6.911939 avg loss: 7.01909314884859\n",
      "[1950] loss: 6.942792 avg loss: 7.0203171711342005\n",
      "[1960] loss: 7.0510445 avg loss: 7.009181462082208\n",
      "[1970] loss: 6.9174805 avg loss: 7.0279788316464895\n",
      "[1980] loss: 7.06576 avg loss: 7.027692102918438\n",
      "[1990] loss: 6.9656725 avg loss: 7.032193726184321\n",
      "[2000] loss: 7.2511797 avg loss: 7.032065157796822\n",
      "[2010] loss: 6.996317 avg loss: 7.0406305462706325\n",
      "[2020] loss: 6.9940453 avg loss: 7.038438881144804\n",
      "[2030] loss: 7.2333055 avg loss: 7.0560289738225\n",
      "[2040] loss: 7.1184025 avg loss: 7.0566382501639575\n",
      "[2050] loss: 6.944876 avg loss: 7.0560923277163035\n",
      "[2060] loss: 6.9800973 avg loss: 7.0417372946645695\n",
      "[2070] loss: 7.1251273 avg loss: 7.031951315262738\n",
      "[2080] loss: 7.0443892 avg loss: 7.0226989633896775\n",
      "[2090] loss: 7.1423473 avg loss: 7.009240795584286\n",
      "[2100] loss: 7.1481857 avg loss: 7.016564967585545\n",
      "[2110] loss: 7.218355 avg loss: 7.009832157808192\n",
      "[2120] loss: 7.058953 avg loss: 7.010549068450928\n",
      "[2130] loss: 7.0795827 avg loss: 7.0229289484959025\n",
      "[2140] loss: 7.1212187 avg loss: 7.016337824802773\n",
      "[2150] loss: 7.1085596 avg loss: 6.996988997739904\n",
      "[2160] loss: 7.368992 avg loss: 7.0094461628035\n",
      "[2170] loss: 6.944812 avg loss: 7.002063956915164\n",
      "[2180] loss: 6.8600945 avg loss: 6.98368044460521\n",
      "[2190] loss: 6.979842 avg loss: 6.997851886001288\n",
      "[2200] loss: 7.208018 avg loss: 7.006702329598221\n",
      "[2210] loss: 6.928687 avg loss: 7.01727809158026\n",
      "[2220] loss: 6.9095426 avg loss: 7.013970066519344\n",
      "[2230] loss: 7.0407405 avg loss: 7.015262884252212\n",
      "[2240] loss: 7.0824494 avg loss: 7.012397466921339\n",
      "[2250] loss: 6.883049 avg loss: 7.0091397528554875\n",
      "[2260] loss: 7.029231 avg loss: 6.996039942199109\n",
      "[2270] loss: 6.901267 avg loss: 6.998450447531307\n",
      "[2280] loss: 7.1295247 avg loss: 7.004388603509641\n",
      "[2290] loss: 6.9809427 avg loss: 7.005481748019948\n",
      "[2300] loss: 7.095477 avg loss: 7.0099932165706855\n",
      "[2310] loss: 7.019586 avg loss: 7.018983401504218\n",
      "[2320] loss: 6.9448004 avg loss: 7.019154314901314\n",
      "[2330] loss: 7.036794 avg loss: 7.023342992745194\n",
      "[2340] loss: 7.01797 avg loss: 7.02493829353183\n",
      "[2350] loss: 7.0607247 avg loss: 7.025644610909855\n",
      "[2360] loss: 6.8427763 avg loss: 7.0200721329333735\n",
      "[2370] loss: 7.0440445 avg loss: 7.019997101204068\n",
      "[2380] loss: 6.8913503 avg loss: 7.0207256242340685\n",
      "[2390] loss: 6.9134808 avg loss: 7.00822722678091\n",
      "[2400] loss: 7.280048 avg loss: 7.006449222564697\n",
      "[2410] loss: 6.8947744 avg loss: 6.997102784175499\n",
      "[2420] loss: 6.965925 avg loss: 6.993297604953542\n",
      "[2430] loss: 7.239051 avg loss: 6.999954008588604\n",
      "[2440] loss: 6.885415 avg loss: 7.006372863171148\n",
      "[2450] loss: 6.9309344 avg loss: 6.99894905090332\n",
      "[2460] loss: 6.8927107 avg loss: 6.984588529549393\n",
      "[2470] loss: 6.9371367 avg loss: 6.9809449233260805\n",
      "[2480] loss: 6.9992704 avg loss: 6.983335149054434\n",
      "[2490] loss: 7.184942 avg loss: 6.975736608692244\n",
      "[2500] loss: 6.901846 avg loss: 6.9814585143444585\n",
      "[2510] loss: 6.937109 avg loss: 6.986552967744715\n",
      "[2520] loss: 6.994673 avg loss: 6.996438521964877\n",
      "[2530] loss: 7.003195 avg loss: 6.9872486731585335\n",
      "[2540] loss: 6.9838357 avg loss: 6.995586750554104\n",
      "[2550] loss: 6.988696 avg loss: 7.004902278675752\n",
      "[2560] loss: 6.922829 avg loss: 7.004846563526228\n",
      "[2570] loss: 6.9941087 avg loss: 7.001829240836349\n",
      "[2580] loss: 6.9279103 avg loss: 7.0023155399397305\n",
      "[2590] loss: 6.9176216 avg loss: 7.0015786769343356\n",
      "[2600] loss: 6.9177914 avg loss: 6.9961111311819035\n",
      "[2610] loss: 6.9166517 avg loss: 6.995602916268742\n",
      "[2620] loss: 6.929892 avg loss: 6.999562646828446\n",
      "[2630] loss: 6.8995976 avg loss: 6.995869926377838\n",
      "[2640] loss: 7.0742197 avg loss: 6.9995428814607505\n",
      "[2650] loss: 7.032054 avg loss: 6.993750721800561\n",
      "[2660] loss: 7.1112814 avg loss: 6.984546932519651\n",
      "[2670] loss: 6.9360633 avg loss: 6.985203238094554\n",
      "[2680] loss: 6.929571 avg loss: 6.989680262172923\n",
      "[2690] loss: 6.916929 avg loss: 6.982576323490517\n",
      "[2700] loss: 7.053933 avg loss: 6.981692388945935\n",
      "[2710] loss: 7.117148 avg loss: 6.99934944452024\n",
      "[2720] loss: 6.9840627 avg loss: 6.991376876831055\n",
      "[2730] loss: 7.0599966 avg loss: 6.985902169171502\n",
      "[2740] loss: 7.00189 avg loss: 6.97596544377944\n",
      "[2750] loss: 7.0497766 avg loss: 6.984982499889299\n",
      "[2760] loss: 6.9155526 avg loss: 6.986475327435662\n",
      "[2770] loss: 7.027969 avg loss: 6.9873934072606705\n",
      "[2780] loss: 6.984286 avg loss: 6.986206512825162\n",
      "[2790] loss: 6.9125824 avg loss: 6.991431563508277\n",
      "[2800] loss: 7.0077467 avg loss: 6.986018882078283\n",
      "[2810] loss: 7.139971 avg loss: 6.981524897556679\n",
      "[2820] loss: 6.8753448 avg loss: 6.976763482187309\n",
      "[2830] loss: 6.9631324 avg loss: 6.9820377592946965\n",
      "[2840] loss: 6.906313 avg loss: 6.993821910783356\n",
      "[2850] loss: 6.931762 avg loss: 7.003743592430563\n",
      "[2860] loss: 7.0188913 avg loss: 7.005231530058618\n",
      "[2870] loss: 7.0865 avg loss: 7.014300065882066\n",
      "[2880] loss: 7.0462704 avg loss: 7.023484276790245\n",
      "[2890] loss: 7.1965837 avg loss: 7.014805335624545\n",
      "[2900] loss: 6.9371905 avg loss: 7.001358714758181\n",
      "[2910] loss: 6.824705 avg loss: 6.998055682462804\n",
      "[2920] loss: 7.0783415 avg loss: 6.992103408364689\n",
      "[2930] loss: 6.9686937 avg loss: 6.979413827260335\n",
      "[2940] loss: 7.08734 avg loss: 6.982897393843707\n",
      "[2950] loss: 7.048152 avg loss: 6.997386418136895\n",
      "[2960] loss: 7.0795383 avg loss: 6.999671982783897\n",
      "[2970] loss: 7.0563335 avg loss: 7.002446829103956\n",
      "[2980] loss: 6.8334394 avg loss: 6.99792470184027\n",
      "[2990] loss: 6.9657235 avg loss: 7.00398281508801\n",
      "[3000] loss: 6.8848524 avg loss: 6.98763604257621\n",
      "[3010] loss: 6.9533076 avg loss: 6.989692996529972\n",
      "[3020] loss: 7.035572 avg loss: 6.990667240292418\n",
      "[3030] loss: 6.907146 avg loss: 6.9883415745753865\n",
      "[3040] loss: 6.97923 avg loss: 6.987420830072141\n",
      "[3050] loss: 7.071221 avg loss: 6.988585743249631\n",
      "[3060] loss: 6.9497547 avg loss: 6.986651037253585\n",
      "[3070] loss: 6.9787674 avg loss: 6.979022558997659\n",
      "[3080] loss: 7.027175 avg loss: 6.978353145075779\n",
      "[3090] loss: 6.889042 avg loss: 6.981817507276348\n",
      "[3100] loss: 7.1248302 avg loss: 6.985584333831189\n",
      "[3110] loss: 6.9382358 avg loss: 6.980109887964585\n",
      "[3120] loss: 7.1570067 avg loss: 6.988445889716055\n",
      "[3130] loss: 6.9092565 avg loss: 6.980623834273395\n",
      "[3140] loss: 6.86794 avg loss: 6.971968921960569\n",
      "[3150] loss: 6.8691 avg loss: 6.963227010240741\n",
      "[3160] loss: 7.070512 avg loss: 6.965653475593118\n",
      "[3170] loss: 6.888985 avg loss: 6.9662272976894\n",
      "[3180] loss: 6.9342227 avg loss: 6.967103256898768\n",
      "[3190] loss: 7.1837115 avg loss: 6.970573060652789\n",
      "[3200] loss: 7.0332956 avg loss: 6.983110736398136\n",
      "[3210] loss: 6.8767357 avg loss: 6.976926261303472\n",
      "[3220] loss: 6.8642483 avg loss: 6.976294162226658\n",
      "[3230] loss: 6.8723593 avg loss: 6.9774392726374606\n",
      "[3240] loss: 6.9343243 avg loss: 6.9767063271765615\n",
      "[3250] loss: 7.002034 avg loss: 6.977399283764409\n",
      "[3260] loss: 6.9825897 avg loss: 6.979859679353003\n",
      "[3270] loss: 7.0085235 avg loss: 6.977844846014883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3280] loss: 6.994136 avg loss: 6.973767018785663\n",
      "[3290] loss: 6.912097 avg loss: 6.974656067642511\n",
      "[3300] loss: 6.9572916 avg loss: 6.970893074484432\n",
      "[3310] loss: 6.8951654 avg loss: 6.967541367399926\n",
      "[3320] loss: 7.190692 avg loss: 6.969088778776281\n",
      "[3330] loss: 6.913355 avg loss: 6.964092123742197\n",
      "[3340] loss: 7.077137 avg loss: 6.969026649699492\n",
      "[3350] loss: 6.8967395 avg loss: 6.971350819456811\n",
      "[3360] loss: 6.9382353 avg loss: 6.974338952232809\n",
      "[3370] loss: 6.883154 avg loss: 6.972779124390845\n",
      "[3380] loss: 6.923872 avg loss: 6.971545172672646\n",
      "[3390] loss: 6.887587 avg loss: 6.962701376746683\n",
      "[3400] loss: 7.2596574 avg loss: 6.96737393210916\n",
      "[3410] loss: 7.0197997 avg loss: 6.968817318187041\n",
      "[3420] loss: 7.12302 avg loss: 6.97922124114691\n",
      "[3430] loss: 6.891024 avg loss: 6.984499622793758\n",
      "[3440] loss: 6.9388814 avg loss: 6.995272599014581\n",
      "[3450] loss: 7.0780854 avg loss: 6.9947135775697\n",
      "[3460] loss: 6.881773 avg loss: 6.981584801393397\n",
      "[3470] loss: 6.950564 avg loss: 6.979498610777013\n",
      "[3480] loss: 7.1193876 avg loss: 6.97769832611084\n",
      "[3490] loss: 6.9508886 avg loss: 6.975307464599609\n",
      "[3500] loss: 7.0455203 avg loss: 6.9791828230315565\n",
      "[3510] loss: 7.123959 avg loss: 6.986319354936188\n",
      "[3520] loss: 7.093114 avg loss: 6.985010838976093\n",
      "[3530] loss: 7.111997 avg loss: 6.989620994119083\n",
      "[3540] loss: 6.8329024 avg loss: 6.979466204549752\n",
      "[3550] loss: 6.876672 avg loss: 6.974104507296693\n",
      "[3560] loss: 7.094103 avg loss: 6.971929353826186\n",
      "[3570] loss: 6.9938197 avg loss: 6.965539792004754\n",
      "[3580] loss: 6.9030223 avg loss: 6.959570669660382\n",
      "[3590] loss: 6.975982 avg loss: 6.955577242608164\n",
      "[3600] loss: 6.994931 avg loss: 6.964411230648265\n",
      "[3610] loss: 6.8855042 avg loss: 6.966868007884306\n",
      "[3620] loss: 7.04512 avg loss: 6.980922914018818\n",
      "[3630] loss: 6.985175 avg loss: 6.982438152911616\n",
      "[3640] loss: 7.011219 avg loss: 6.990938663482666\n",
      "[3650] loss: 7.040449 avg loss: 6.985183080037435\n",
      "[3660] loss: 7.1523848 avg loss: 6.989422283920587\n",
      "[3670] loss: 7.1500406 avg loss: 6.982522964477539\n",
      "[3680] loss: 6.8612843 avg loss: 6.974832235598097\n",
      "[3690] loss: 6.896669 avg loss: 6.972851940229828\n",
      "[3700] loss: 7.0916505 avg loss: 6.985782950532203\n",
      "[3710] loss: 7.1053195 avg loss: 6.991796998416676\n",
      "[3720] loss: 6.8782525 avg loss: 7.000201468374215\n",
      "[3730] loss: 6.929883 avg loss: 6.989451380336986\n",
      "[3740] loss: 6.8614717 avg loss: 6.986036207161698\n",
      "[3750] loss: 6.9527154 avg loss: 6.9809930277805705\n",
      "[3760] loss: 7.083212 avg loss: 6.9697444672678035\n",
      "[3770] loss: 6.937194 avg loss: 6.951590023788751\n",
      "[3780] loss: 6.990687 avg loss: 6.971118618460262\n",
      "[3790] loss: 6.9230733 avg loss: 6.9775735724206065\n",
      "[3800] loss: 7.0526505 avg loss: 6.982742403067794\n",
      "[3810] loss: 7.045571 avg loss: 6.979277816473269\n",
      "[3820] loss: 6.856589 avg loss: 6.9870716917748545\n",
      "[3830] loss: 6.8971386 avg loss: 6.978052784414852\n",
      "[3840] loss: 6.93652 avg loss: 6.977151833328546\n",
      "[3850] loss: 6.950909 avg loss: 6.981640899882597\n",
      "[3860] loss: 7.067815 avg loss: 6.992618336397059\n",
      "[3870] loss: 7.0850267 avg loss: 6.9750411837708715\n",
      "[3880] loss: 6.920268 avg loss: 6.9690634877074\n",
      "[3890] loss: 7.002434 avg loss: 6.973016234005199\n",
      "[3900] loss: 6.96901 avg loss: 6.964923437903909\n",
      "[3910] loss: 7.0806665 avg loss: 6.963955271477793\n",
      "[3920] loss: 7.009738 avg loss: 6.9732505760940855\n",
      "[3930] loss: 7.031177 avg loss: 6.98769853629318\n",
      "[3940] loss: 6.900105 avg loss: 6.97819839739332\n",
      "[3950] loss: 6.9111013 avg loss: 6.975362356971292\n",
      "[3960] loss: 7.0803404 avg loss: 6.981302392249014\n",
      "[3970] loss: 6.9444575 avg loss: 6.979270374073701\n",
      "[3980] loss: 6.9704266 avg loss: 6.97368569467582\n",
      "[3990] loss: 6.973605 avg loss: 6.978860892501532\n",
      "[4000] loss: 7.0266232 avg loss: 6.978200454337924\n",
      "[4010] loss: 6.9086733 avg loss: 6.977862058901319\n",
      "[4020] loss: 6.9565063 avg loss: 6.976013361238966\n",
      "[4030] loss: 7.08366 avg loss: 6.968804078943589\n",
      "[4040] loss: 6.878378 avg loss: 6.963438305200315\n",
      "[4050] loss: 6.9124207 avg loss: 6.96630682664759\n",
      "[4060] loss: 6.8868275 avg loss: 6.956492545557957\n",
      "[4070] loss: 7.135993 avg loss: 6.9661192519992\n",
      "[4080] loss: 6.965493 avg loss: 6.980880325915766\n",
      "[4090] loss: 7.001133 avg loss: 6.978489660749249\n",
      "[4100] loss: 6.909593 avg loss: 6.975671066957362\n",
      "[4110] loss: 6.9660034 avg loss: 6.985280551162421\n",
      "[4120] loss: 6.8981724 avg loss: 6.981389223360548\n",
      "[4130] loss: 7.0292397 avg loss: 6.973059887979545\n",
      "[4140] loss: 6.9466453 avg loss: 6.975865840911865\n",
      "[4150] loss: 6.9388433 avg loss: 6.978733315187342\n",
      "[4160] loss: 6.8922343 avg loss: 6.9730312216515635\n",
      "[4170] loss: 6.863788 avg loss: 6.975377802755318\n",
      "[4180] loss: 6.8669195 avg loss: 6.975391285092223\n",
      "[4190] loss: 6.8973236 avg loss: 6.956214278352027\n",
      "[4200] loss: 7.0329103 avg loss: 6.947676284640443\n",
      "[4210] loss: 6.9783363 avg loss: 6.952115956474753\n",
      "[4220] loss: 6.904186 avg loss: 6.946665576860016\n",
      "[4230] loss: 7.0989156 avg loss: 6.957850699331246\n",
      "[4240] loss: 6.9961276 avg loss: 6.97513479344985\n",
      "[4250] loss: 6.907334 avg loss: 6.974381484237372\n",
      "[4260] loss: 7.15745 avg loss: 6.967290513655719\n",
      "[4270] loss: 7.0743628 avg loss: 6.957780651017731\n",
      "[4280] loss: 6.888959 avg loss: 6.955578486124675\n",
      "[4290] loss: 6.9086366 avg loss: 6.955471777448468\n",
      "[4300] loss: 7.0172873 avg loss: 6.970371760574042\n",
      "[4310] loss: 7.1368184 avg loss: 6.983457181967941\n",
      "[4320] loss: 6.921499 avg loss: 6.988397673064587\n",
      "[4330] loss: 7.061789 avg loss: 6.971998149273443\n",
      "[4340] loss: 7.1948133 avg loss: 6.963256031859155\n",
      "[4350] loss: 6.903052 avg loss: 6.952610791898241\n",
      "[4360] loss: 6.866058 avg loss: 6.946858808106067\n",
      "[4370] loss: 6.9470797 avg loss: 6.9547994744543935\n",
      "[4380] loss: 6.9745255 avg loss: 6.962584841485117\n",
      "[4390] loss: 6.943549 avg loss: 6.966850757598877\n",
      "[4400] loss: 6.919087 avg loss: 6.963965789944518\n",
      "[4410] loss: 6.9542627 avg loss: 6.966548517638562\n",
      "[4420] loss: 7.014138 avg loss: 6.957195665322098\n",
      "[4430] loss: 7.025719 avg loss: 6.956902111277861\n",
      "[4440] loss: 6.880553 avg loss: 6.950330360263002\n",
      "[4450] loss: 6.8870473 avg loss: 6.952993617338293\n",
      "[4460] loss: 6.892936 avg loss: 6.951223588457294\n",
      "[4470] loss: 6.861333 avg loss: 6.948647489734724\n",
      "[4480] loss: 6.9958525 avg loss: 6.952795140883502\n",
      "[4490] loss: 6.9093785 avg loss: 6.96585657082352\n",
      "[4500] loss: 6.8951645 avg loss: 6.969430689718209\n",
      "[4510] loss: 7.0797634 avg loss: 6.964960892995198\n",
      "[4520] loss: 7.040844 avg loss: 6.975075656292486\n",
      "[4530] loss: 6.9316325 avg loss: 6.982465201733159\n",
      "[4540] loss: 7.1226363 avg loss: 6.966733857697132\n",
      "[4550] loss: 7.11202 avg loss: 6.96649377486285\n",
      "[4560] loss: 6.8977833 avg loss: 6.9704650991103225\n",
      "[4570] loss: 7.1007376 avg loss: 6.961681908252192\n",
      "[4580] loss: 6.8808336 avg loss: 6.956168165393904\n",
      "[4590] loss: 6.9845095 avg loss: 6.964560490028531\n",
      "[4600] loss: 6.8933153 avg loss: 6.955071458629534\n",
      "[4610] loss: 6.933094 avg loss: 6.954073064467487\n",
      "[4620] loss: 6.892011 avg loss: 6.962752061731675\n",
      "[4630] loss: 7.013891 avg loss: 6.952765109492283\n",
      "[4640] loss: 7.077947 avg loss: 6.953132610695035\n",
      "[4650] loss: 6.904997 avg loss: 6.948813905902937\n",
      "[4660] loss: 6.851059 avg loss: 6.947342105940277\n",
      "[4670] loss: 6.9517474 avg loss: 6.9413154078464885\n",
      "[4680] loss: 6.8498263 avg loss: 6.939348557416131\n",
      "[4690] loss: 6.9654684 avg loss: 6.936767802518957\n",
      "[4700] loss: 6.8376384 avg loss: 6.939111718944475\n",
      "[4710] loss: 6.96685 avg loss: 6.939334429946601\n",
      "[4720] loss: 6.975414 avg loss: 6.938209683287377\n",
      "[4730] loss: 6.8508606 avg loss: 6.944379479277368\n",
      "[4740] loss: 6.937291 avg loss: 6.950531361149807\n",
      "[4750] loss: 6.9741726 avg loss: 6.947593530019124\n",
      "[4760] loss: 7.027693 avg loss: 6.953833234076407\n",
      "[4770] loss: 6.891941 avg loss: 6.9517290358449895\n",
      "[4780] loss: 6.935621 avg loss: 6.945386811798694\n",
      "[4790] loss: 6.9484787 avg loss: 6.958094363119088\n",
      "[4800] loss: 6.9382224 avg loss: 6.972409061357086\n",
      "[4810] loss: 6.884607 avg loss: 6.974520907682531\n",
      "[4820] loss: 6.985013 avg loss: 6.975340085871079\n",
      "[4830] loss: 7.159383 avg loss: 6.983894011553596\n",
      "[4840] loss: 6.8535047 avg loss: 6.973080176933139\n",
      "[4850] loss: 6.898035 avg loss: 6.957741550370758\n",
      "[4860] loss: 6.918241 avg loss: 6.9507891804564235\n",
      "[4870] loss: 6.903311 avg loss: 6.950286790436389\n",
      "[4880] loss: 6.862678 avg loss: 6.946535895852482\n",
      "[4890] loss: 6.9146376 avg loss: 6.941673839793486\n",
      "[4900] loss: 6.897757 avg loss: 6.954883949429381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4910] loss: 6.884277 avg loss: 6.9592940012613935\n",
      "[4920] loss: 6.9139204 avg loss: 6.961784222546746\n",
      "[4930] loss: 6.9341273 avg loss: 6.962413469950358\n",
      "[4940] loss: 7.0523176 avg loss: 6.96344170850866\n",
      "[4950] loss: 6.924069 avg loss: 6.96117275836421\n",
      "[4960] loss: 6.8916125 avg loss: 6.964246553533218\n",
      "[4970] loss: 6.8813887 avg loss: 6.97261214723774\n",
      "[4980] loss: 6.9764614 avg loss: 6.973466265435312\n",
      "[4990] loss: 7.0100455 avg loss: 6.982722039316215\n",
      "[5000] loss: 6.941612 avg loss: 6.977171196657069\n",
      "[5010] loss: 6.8972545 avg loss: 6.973349889119466\n",
      "[5020] loss: 6.839958 avg loss: 6.973565550411449\n",
      "[5030] loss: 6.897233 avg loss: 6.978446866951737\n",
      "[5040] loss: 6.911374 avg loss: 6.9757006028119255\n",
      "[5050] loss: 6.9306903 avg loss: 6.966528387630687\n",
      "[5060] loss: 6.92951 avg loss: 6.963967398101208\n",
      "[5070] loss: 6.937918 avg loss: 6.965164811003442\n",
      "[5080] loss: 6.880328 avg loss: 6.955226243710985\n",
      "[5090] loss: 6.931416 avg loss: 6.9602791000815\n",
      "[5100] loss: 6.9977074 avg loss: 6.973451408685422\n",
      "[5110] loss: 6.9537663 avg loss: 6.984595111772125\n",
      "[5120] loss: 6.890358 avg loss: 6.974931193333046\n",
      "[5130] loss: 6.8813467 avg loss: 6.982382409712848\n",
      "[5140] loss: 6.9160037 avg loss: 6.977282075320973\n",
      "[5150] loss: 6.8806496 avg loss: 6.965768514894972\n",
      "[5160] loss: 6.868461 avg loss: 6.952530159669764\n",
      "[5170] loss: 6.7535973 avg loss: 6.945839554655786\n",
      "[5180] loss: 6.904891 avg loss: 6.94406795501709\n",
      "[5190] loss: 6.8900714 avg loss: 6.943636118196974\n",
      "[5200] loss: 7.078576 avg loss: 6.948874800812964\n",
      "[5210] loss: 6.918877 avg loss: 6.9521241001054355\n",
      "[5220] loss: 7.011739 avg loss: 6.950345338559618\n",
      "[5230] loss: 6.8085847 avg loss: 6.948353421454336\n",
      "[5240] loss: 6.9608583 avg loss: 6.948469853868671\n",
      "[5250] loss: 6.9193726 avg loss: 6.948179815329757\n",
      "[5260] loss: 6.9171333 avg loss: 6.941822631686342\n",
      "[5270] loss: 6.9204807 avg loss: 6.956901363298004\n",
      "[5280] loss: 6.933666 avg loss: 6.959045503653732\n",
      "[5290] loss: 7.0161715 avg loss: 6.966214123894186\n",
      "[5300] loss: 6.856086 avg loss: 6.954030495063932\n",
      "[5310] loss: 6.892644 avg loss: 6.954236787908218\n",
      "[5320] loss: 6.846681 avg loss: 6.954698945961747\n",
      "[5330] loss: 6.9304113 avg loss: 6.951161608976476\n",
      "[5340] loss: 6.986679 avg loss: 6.94184113483803\n",
      "[5350] loss: 7.0128665 avg loss: 6.958623474719477\n",
      "[5360] loss: 6.905102 avg loss: 6.962466875712077\n",
      "[5370] loss: 6.8494706 avg loss: 6.950290866926605\n",
      "[5380] loss: 7.1085234 avg loss: 6.962469241198371\n",
      "[5390] loss: 6.9476433 avg loss: 6.978272372601079\n",
      "[5400] loss: 6.9385858 avg loss: 6.983287829978793\n",
      "[5410] loss: 6.989736 avg loss: 6.975910270915312\n",
      "[5420] loss: 6.9134116 avg loss: 6.97421958399754\n",
      "[5430] loss: 6.8226833 avg loss: 6.976281652263567\n",
      "[5440] loss: 7.0165844 avg loss: 6.9714771439047425\n",
      "[5450] loss: 6.8678794 avg loss: 6.95472967858408\n",
      "[5460] loss: 6.8771157 avg loss: 6.957588728736429\n",
      "[5470] loss: 6.9448724 avg loss: 6.9563653422336955\n",
      "[5480] loss: 6.987484 avg loss: 6.95021624658622\n",
      "[5490] loss: 6.84644 avg loss: 6.946071951997046\n",
      "[5500] loss: 6.888094 avg loss: 6.952675445407045\n",
      "[5510] loss: 7.006497 avg loss: 6.945346383487477\n",
      "[5520] loss: 6.855001 avg loss: 6.946710249956916\n",
      "[5530] loss: 6.863368 avg loss: 6.947808733173445\n",
      "[5540] loss: 6.884567 avg loss: 6.938934952605004\n",
      "[5550] loss: 6.8487797 avg loss: 6.929405427446552\n",
      "[5560] loss: 6.984165 avg loss: 6.9336993367064235\n",
      "[5570] loss: 7.093231 avg loss: 6.930213498134239\n",
      "[5580] loss: 6.904784 avg loss: 6.929145756889792\n",
      "[5590] loss: 6.91161 avg loss: 6.9271379265130735\n",
      "[5600] loss: 6.9010067 avg loss: 6.923581796533921\n",
      "[5610] loss: 6.9963174 avg loss: 6.935520518059824\n",
      "[5620] loss: 6.8156486 avg loss: 6.941706152523265\n",
      "[5630] loss: 6.9042783 avg loss: 6.939294235379088\n",
      "[5640] loss: 6.900663 avg loss: 6.945565850126977\n",
      "[5650] loss: 7.191544 avg loss: 6.959068821925743\n",
      "[5660] loss: 6.929679 avg loss: 6.960480231864779\n",
      "[5670] loss: 6.9398007 avg loss: 6.962235226350672\n",
      "[5680] loss: 6.8556824 avg loss: 6.953273689045625\n",
      "[5690] loss: 6.9858356 avg loss: 6.964351700801475\n",
      "[5700] loss: 6.780282 avg loss: 6.955771362080293\n",
      "[5710] loss: 6.9808655 avg loss: 6.944019841212852\n",
      "[5720] loss: 6.8848295 avg loss: 6.945685302509981\n",
      "[5730] loss: 7.0679007 avg loss: 6.952141079248166\n",
      "[5740] loss: 6.8951635 avg loss: 6.94481808531518\n",
      "[5750] loss: 6.93062 avg loss: 6.948454941020293\n",
      "[5760] loss: 7.0311084 avg loss: 6.9629753430684405\n",
      "[5770] loss: 6.8725586 avg loss: 6.9553925009334785\n",
      "[5780] loss: 6.9902983 avg loss: 6.953966524086747\n",
      "[5790] loss: 6.968817 avg loss: 6.956007723714791\n",
      "[5800] loss: 7.0105157 avg loss: 6.95919438904407\n",
      "[5810] loss: 6.9599376 avg loss: 6.951443681529924\n",
      "[5820] loss: 7.0658345 avg loss: 6.947497199563419\n",
      "[5830] loss: 7.0054765 avg loss: 6.949907854491589\n",
      "[5840] loss: 7.0399714 avg loss: 6.943570006127451\n",
      "[5850] loss: 6.910487 avg loss: 6.948368353002212\n",
      "[5860] loss: 6.813067 avg loss: 6.95300246220009\n",
      "[5870] loss: 6.902901 avg loss: 6.943769698049508\n",
      "[5880] loss: 6.867158 avg loss: 6.956014109592812\n",
      "[5890] loss: 6.8761744 avg loss: 6.954604653751149\n",
      "[5900] loss: 6.8661804 avg loss: 6.948533553703158\n",
      "[5910] loss: 6.901289 avg loss: 6.939771511975457\n",
      "[5920] loss: 7.013022 avg loss: 6.953069172653497\n"
     ]
    }
   ],
   "source": [
    "losses = Float64[]\n",
    "\n",
    "# We create ops to push data in and pull data out of the TPU\n",
    "function make_placeholder(sess, device, len)\n",
    "    return with_device(XLA.tf_host_cpu_device(sess, device)) do\n",
    "        return XLA.placeholder(UInt32, shape=(len,))\n",
    "    end\n",
    "end\n",
    "\n",
    "x_size = 224*224*batch_size\n",
    "y_size = batch_size\n",
    "x_placeholder = make_placeholder(sess, tpu_device, x_size)\n",
    "y_placeholder = make_placeholder(sess, tpu_device, y_size)\n",
    "\n",
    "infeed_op = XLA.make_infeed_on(sess,\n",
    "    # Make the infeed on this device\n",
    "    tpu_device,\n",
    "    \n",
    "    # It feeds in these element types\n",
    "    (UInt32, UInt32),\n",
    "    \n",
    "    # With these sizes\n",
    "    ((x_size,),(y_size,)),\n",
    "    \n",
    "    # And is associated with these TF placeholders\n",
    "    [x_placeholder, y_placeholder],\n",
    ")\n",
    "\n",
    "# Also make an outfeed op\n",
    "outfeed_op = XLA.make_outfeed_on(sess,\n",
    "    # On this device\n",
    "    tpu_device,\n",
    "    \n",
    "    # Which will output this type\n",
    "    Tuple{XRTArray{Float32, (), 0},}\n",
    ")\n",
    "\n",
    "# For each minibatch\n",
    "for batch_idx in 1:num_batches\n",
    "    batch_slice = permutation[((batch_idx - 1)*batch_size + 1):batch_idx*batch_size]\n",
    "\n",
    "    # Convert the next minibatch of data into our UInt32 vectors:\n",
    "    x_batch = pixel_pack(Xs[:, :, :, batch_slice])\n",
    "    y_batch = UInt32.(Ys[batch_slice])\n",
    "\n",
    "    # Send our tensors in to the predefined placeholders\n",
    "    infeed_dict = Dict(\n",
    "        x_placeholder => vec(x_batch),\n",
    "        y_placeholder => vec(y_batch),\n",
    "    )\n",
    "    run(sess, infeed_op, infeed_dict)\n",
    "    \n",
    "    # Get loss back\n",
    "    loss = run(sess, outfeed_op)\n",
    "    push!(losses, loss)\n",
    "\n",
    "    # Print it out as we go, showing the average loss to (hopefully) watch it decrease\n",
    "    if batch_idx % 10 == 0\n",
    "        println(\"[$batch_idx] loss: $(loss) avg loss: $(mean(losses[max(end-50,1):end]))\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error in running finalizer: ErrorException(\"type TPUSession has no field ptr\")\n",
      "error in running finalizer: ErrorException(\"type TPUSession has no field ptr\")\n",
      "error in running finalizer: ErrorException(\"type TPUSession has no field ptr\")\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip0600\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0601\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip0601)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0602\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip0601)\" points=\"\n",
       "211.005,1440.48 2321.26,1440.48 2321.26,47.2441 211.005,47.2441 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0603\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"2111\" height=\"1394\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  269.733,1440.48 269.733,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  767.933,1440.48 767.933,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1266.13,1440.48 1266.13,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1764.33,1440.48 1764.33,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2262.53,1440.48 2262.53,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.005,1311.02 2321.26,1311.02 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.005,1123.95 2321.26,1123.95 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.005,936.871 2321.26,936.871 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.005,749.797 2321.26,749.797 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.005,562.723 2321.26,562.723 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.005,375.649 2321.26,375.649 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.005,188.575 2321.26,188.575 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.005,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.005,1440.48 211.005,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  269.733,1440.48 269.733,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  767.933,1440.48 767.933,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1266.13,1440.48 1266.13,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1764.33,1440.48 1764.33,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2262.53,1440.48 2262.53,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.005,1311.02 242.659,1311.02 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.005,1123.95 242.659,1123.95 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.005,936.871 242.659,936.871 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.005,749.797 242.659,749.797 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.005,562.723 242.659,562.723 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.005,375.649 242.659,375.649 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.005,188.575 242.659,188.575 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 269.733, 1494.48)\" x=\"269.733\" y=\"1494.48\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 767.933, 1494.48)\" x=\"767.933\" y=\"1494.48\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1266.13, 1494.48)\" x=\"1266.13\" y=\"1494.48\">1000</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1764.33, 1494.48)\" x=\"1764.33\" y=\"1494.48\">1500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2262.53, 1494.48)\" x=\"2262.53\" y=\"1494.48\">2000</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 187.005, 1328.52)\" x=\"187.005\" y=\"1328.52\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 187.005, 1141.45)\" x=\"187.005\" y=\"1141.45\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 187.005, 954.371)\" x=\"187.005\" y=\"954.371\">6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 187.005, 767.297)\" x=\"187.005\" y=\"767.297\">7</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 187.005, 580.223)\" x=\"187.005\" y=\"580.223\">8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 187.005, 393.149)\" x=\"187.005\" y=\"393.149\">9</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 187.005, 206.075)\" x=\"187.005\" y=\"206.075\">10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1266.13, 1590.4)\" x=\"1266.13\" y=\"1590.4\">Minibatch</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 743.863)\" x=\"57.6\" y=\"743.863\">Loss</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip0603)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  270.729,874.683 271.726,917.955 272.722,958.079 273.718,984.838 274.715,1026.69 275.711,1068.1 276.708,1099.1 277.704,1145.6 278.7,1181.44 279.697,1216.65 \n",
       "  280.693,555.321 281.69,440.661 282.686,476.697 283.682,514.272 284.679,547.842 285.675,586.101 286.672,624.58 287.668,659.39 288.664,689.883 289.661,727.263 \n",
       "  290.657,866.617 291.654,982.449 292.65,1011.72 293.646,1052.36 294.643,1081.8 295.639,1126.69 296.636,1149.8 297.632,1193.71 298.628,1215.22 299.625,1243.12 \n",
       "  300.621,856.544 301.618,485.821 302.614,519.894 303.61,559.497 304.607,594.469 305.603,629.655 306.6,662.206 307.596,700.226 308.592,730.574 309.589,768.472 \n",
       "  310.585,709.98 311.582,575.96 312.578,610.541 313.574,643.975 314.571,677.594 315.567,714.338 316.564,745.449 317.56,774.826 318.556,808.986 319.553,847.006 \n",
       "  320.549,778.134 321.546,479.571 322.542,516.657 323.538,555.813 324.535,585.874 325.531,605.041 326.528,647.571 327.524,682.947 328.52,717.821 329.517,749.241 \n",
       "  330.513,747.815 331.51,242.14 332.506,266.717 333.502,306.824 334.499,344.309 335.495,371.226 336.492,406.505 337.488,434.99 338.484,465.579 339.481,504.187 \n",
       "  340.477,528.787 341.474,540.891 342.47,566.411 343.466,600.248 344.463,631.146 345.459,661.958 346.456,696.871 347.452,724.905 348.448,762.708 349.445,790.198 \n",
       "  350.441,819.807 351.438,801.184 352.434,797.536 353.43,829.631 354.427,865.919 355.423,896.068 356.42,932.339 357.416,963.247 358.412,996.615 359.409,1027.63 \n",
       "  360.405,1059.9 361.402,936.403 362.398,877.146 363.394,909.834 364.391,952.131 365.387,979.959 366.384,1015.6 367.38,1052.73 368.376,1095.01 369.373,1127.89 \n",
       "  370.369,1163.81 371.366,1001.36 372.362,790.532 373.358,827.327 374.355,860.911 375.351,895.716 376.348,932.299 377.344,965.399 378.34,999.911 379.337,1043.24 \n",
       "  380.333,1075.35 381.33,979.215 382.326,625.615 383.322,666.308 384.319,699.962 385.315,738.821 386.312,774.167 387.308,814.164 388.304,840.86 389.301,882.822 \n",
       "  390.297,918.71 391.294,917.107 392.29,585.486 393.286,618.464 394.283,659.15 395.279,694.055 396.276,731.85 397.272,773.758 398.268,803.493 399.265,836.927 \n",
       "  400.261,875.43 401.258,912.831 402.254,738.965 403.25,769.975 404.247,803.658 405.243,840.298 406.24,878.246 407.236,910.254 408.232,951.499 409.229,986.206 \n",
       "  410.225,1008.43 411.222,1060.59 412.218,1031.11 413.214,1040.89 414.211,1084.51 415.207,1107.6 416.204,1145.79 417.2,1193.47 418.196,1223.79 419.193,1265.61 \n",
       "  420.189,1295.44 421.186,1329.42 422.182,791.43 423.178,489.406 424.175,535.289 425.171,565.663 426.168,605.033 427.164,638.722 428.16,677.398 429.157,717.092 \n",
       "  430.153,749.501 431.15,789.705 432.146,717.429 433.142,646.524 434.139,685.87 435.135,722.295 436.132,759.055 437.128,797.482 438.124,834.348 439.121,874.567 \n",
       "  440.117,913.934 441.114,947.761 442.11,846.72 443.106,602.13 444.103,640.973 445.099,678.074 446.096,718.511 447.092,751.751 448.088,788.639 449.085,825.711 \n",
       "  450.081,869.218 451.078,904.519 452.074,891.758 453.07,793.362 454.067,834.876 455.063,869.633 456.06,909.234 457.056,942.547 458.052,980.686 459.049,1013.81 \n",
       "  460.045,1056.51 461.042,1091.15 462.038,1119.48 463.034,734.99 464.031,775.787 465.027,811.023 466.024,848.582 467.02,888.143 468.016,924.696 469.013,962.902 \n",
       "  470.009,996.105 471.006,1041.7 472.002,1072.82 472.998,356.756 473.995,258.524 474.991,291.696 475.988,334.364 476.984,364.097 477.98,405.803 478.977,449.973 \n",
       "  479.973,478.941 480.97,517.252 481.966,552.633 482.962,541.817 483.959,528.119 484.955,568.327 485.952,600.068 486.948,641.541 487.944,679.369 488.941,713.486 \n",
       "  489.937,752.744 490.934,788.083 491.93,829.076 492.926,696.136 493.923,597.484 494.919,639.393 495.916,674.278 496.912,713.65 497.908,748.294 498.905,790.999 \n",
       "  499.901,825.556 500.898,861.028 501.894,902.033 502.89,763.751 503.887,498.277 504.883,532.305 505.88,577.327 506.876,610.73 507.872,646.618 508.869,679.561 \n",
       "  509.865,719.09 510.861,758.312 511.858,789.939 512.854,698.899 513.851,275.797 514.847,316.343 515.843,350.357 516.84,389.146 517.836,428.89 518.833,449.268 \n",
       "  519.829,485.158 520.825,529.75 521.822,552.142 522.818,604.013 523.815,730.857 524.811,769.016 525.807,793.542 526.804,826.478 527.8,856.205 528.797,884.151 \n",
       "  529.793,921.672 530.789,949.834 531.786,980.677 532.782,1008.14 533.779,770.259 534.775,783.043 535.771,820.011 536.768,846.688 537.764,886.934 538.761,917.065 \n",
       "  539.757,945.97 540.753,982.579 541.75,1011.49 542.746,1051.21 543.743,749.777 544.739,680.359 545.735,711.986 546.732,744.744 547.728,779.203 548.725,812.153 \n",
       "  549.721,840.809 550.717,869.501 551.714,905.973 552.71,934.761 553.707,689.463 554.703,535.663 555.699,558.273 556.696,591.815 557.692,631.238 558.689,661.418 \n",
       "  559.685,695.609 560.681,723.602 561.678,758.54 562.674,789.689 563.671,952.334 564.667,1122.04 565.663,1145.67 566.66,1180.02 567.656,1221.98 568.653,1250.96 \n",
       "  569.649,1292.2 570.645,1328.85 571.642,1357.53 572.638,1401.05 573.635,1267.16 574.631,867.524 575.627,902.89 576.624,938.125 577.62,969.709 578.617,1002.34 \n",
       "  579.613,1041.95 580.609,1076.45 581.606,1109.34 582.602,1143.16 583.599,1113.01 584.595,818.957 585.591,850.379 586.588,886.667 587.584,923.7 588.581,961.518 \n",
       "  589.577,996.791 590.573,1033.69 591.57,1071.14 592.566,1109.41 593.563,1148.31 594.559,624.605 595.555,657.863 596.552,696.655 597.548,731.09 598.545,764.999 \n",
       "  599.541,803.822 600.537,835.998 601.534,874.39 602.53,913.289 603.527,946.636 604.523,657.95 605.519,626.535 606.516,659.71 607.512,694.086 608.509,731.749 \n",
       "  609.505,766.826 610.501,803.711 611.498,839.421 612.494,874.311 613.491,909.431 614.487,807.376 615.483,767.651 616.48,804.867 617.476,841.964 618.473,874.47 \n",
       "  619.469,909.35 620.465,951.535 621.462,980.065 622.458,1025.2 623.455,1064.61 624.451,689.938 625.447,284.334 626.444,325.469 627.44,364.285 628.437,399.641 \n",
       "  629.433,439.067 630.429,476.177 631.426,508.384 632.422,548.199 633.419,588.75 634.415,532.583 635.411,416.831 636.408,446.187 637.404,474.665 638.401,512.728 \n",
       "  639.397,556.062 640.393,584.652 641.39,621.202 642.386,650.004 643.383,690.565 644.379,704.023 645.375,667.548 646.372,709.658 647.368,737.704 648.365,776.284 \n",
       "  649.361,802.563 650.357,833.729 651.354,868.302 652.35,898.522 653.347,932.44 654.343,956.899 655.339,838.213 656.336,875.642 657.332,906.237 658.329,940.08 \n",
       "  659.325,971.3 660.321,1010.13 661.318,1040.86 662.314,1077.02 663.311,1110.89 664.307,1145.32 665.303,942.962 666.3,955.575 667.296,984.156 668.293,1027.43 \n",
       "  669.289,1061 670.285,1099.41 671.282,1128.33 672.278,1162.01 673.275,1194.69 674.271,1234.45 675.267,740.551 676.264,587.049 677.26,616.443 678.257,649.271 \n",
       "  679.253,687.94 680.249,721.294 681.246,756.53 682.242,792.069 683.239,827.897 684.235,859.878 685.231,772.083 686.228,682.291 687.224,715.173 688.221,750.716 \n",
       "  689.217,784.2 690.213,818.364 691.21,853.425 692.206,890.382 693.203,920.992 694.199,958.767 695.195,845.822 696.192,667.43 697.188,701.588 698.185,735.187 \n",
       "  699.181,768.125 700.177,807.645 701.174,841.026 702.17,875.325 703.167,900.17 704.163,934.122 705.159,844.233 706.156,509.174 707.152,557.816 708.149,592.274 \n",
       "  709.145,625.656 710.141,657.351 711.138,692.27 712.134,726.498 713.131,755.196 714.127,659.327 715.123,566.1 716.12,593.765 717.116,636.126 718.113,669.976 \n",
       "  719.109,704.41 720.105,736.027 721.102,768.191 722.098,796.097 723.095,834.406 724.091,797.595 725.087,724.675 726.084,762.461 727.08,792.225 728.077,825.896 \n",
       "  729.073,856.135 730.069,891.43 731.066,919.039 732.062,955.976 733.059,992.545 734.055,897.777 735.051,630.108 736.048,660.055 737.044,692.417 738.041,730.796 \n",
       "  739.037,768.207 740.033,797.673 741.03,832.507 742.026,863.925 743.023,899.674 744.019,905.308 745.015,521.874 746.012,549.068 747.008,584.374 748.005,615.418 \n",
       "  749.001,642.848 749.997,683.232 750.994,719.008 751.99,743.99 752.987,784.902 753.983,811.431 754.979,901.059 755.976,936.943 756.972,963.922 757.969,999.226 \n",
       "  758.965,1031.94 759.961,1063.05 760.958,1101.44 761.954,1128.88 762.951,1169.52 763.947,1202.47 764.943,901.404 765.94,826.332 766.936,862.474 767.933,895.712 \n",
       "  768.929,929.412 769.925,962.527 770.922,999.163 771.918,1035.51 772.915,1070.19 773.911,1100.71 774.907,777.167 775.904,605.329 776.9,639.996 777.897,675.116 \n",
       "  778.893,708.046 779.889,742.454 780.886,782.593 781.882,812.491 782.879,847.538 783.875,886.413 784.871,838.81 785.868,802.362 786.864,841.677 787.861,878.393 \n",
       "  788.857,914.733 789.853,954.645 790.85,991.376 791.846,1025.76 792.843,1062.74 793.839,1099.61 794.835,695.424 795.832,347.145 796.828,386.81 797.825,423.354 \n",
       "  798.821,442.031 799.817,498.919 800.814,518.119 801.81,552.47 802.807,594.797 803.803,627.468 804.799,645.343 805.796,667.227 806.792,702.976 807.789,728.086 \n",
       "  808.785,766.603 809.781,798.767 810.778,830.179 811.774,861.969 812.771,898.093 813.767,924.276 814.763,748.21 815.76,86.6754 816.756,140.216 817.753,174.267 \n",
       "  818.749,205.309 819.745,239.045 820.742,271.861 821.738,312.015 822.735,342.002 823.731,365.031 824.727,431.305 825.724,695.244 826.72,725.423 827.717,756.26 \n",
       "  828.713,789.6 829.709,810.22 830.706,851.16 831.702,872.414 832.699,899.387 833.695,929.738 834.691,967.484 835.688,744.193 836.684,765.001 837.681,797.445 \n",
       "  838.677,824.477 839.673,863.289 840.67,892.122 841.666,915.102 842.663,950.997 843.659,980.558 844.655,1009.51 845.652,839.092 846.648,793.25 847.645,819.551 \n",
       "  848.641,852.957 849.637,877.45 850.634,914.637 851.63,945.451 852.627,977.469 853.623,1007.57 854.619,1038.74 855.616,715.588 856.612,535.004 857.609,569.772 \n",
       "  858.605,597.388 859.601,624.327 860.598,657.011 861.594,692.382 862.591,717.988 863.587,753.728 864.583,781.416 865.58,793.013 866.576,814.117 867.573,848.152 \n",
       "  868.569,876.135 869.565,907.294 870.562,938.381 871.558,970.627 872.555,997.812 873.551,1029.85 874.547,1064.12 875.544,943.817 876.54,650.792 877.537,672.964 \n",
       "  878.533,700.622 879.529,737.059 880.526,767.082 881.522,798.707 882.519,825.99 883.515,862.957 884.511,888.339 885.508,890.87 886.504,761.725 887.501,792.874 \n",
       "  888.497,819.707 889.493,852.819 890.49,887.886 891.486,916.581 892.483,944.87 893.479,977.695 894.475,1012.76 895.472,1045.51 896.468,806.705 897.465,835.56 \n",
       "  898.461,866.161 899.457,903.365 900.454,938.614 901.45,965.949 902.447,1006.6 903.443,1034.33 904.439,845.441 905.436,768.929 906.432,802.664 907.429,832.127 \n",
       "  908.425,867.751 909.421,898.514 910.418,935.852 911.414,975.062 912.411,1005.71 913.407,1039.37 914.403,1005.37 915.4,959.671 916.396,990.761 917.393,1030.76 \n",
       "  918.389,1062.54 919.385,1094.68 920.382,1124.1 921.378,1163.26 922.375,1200 923.371,1232.92 924.367,984.607 925.364,445.08 926.36,482.146 927.357,515.784 \n",
       "  928.353,559.164 929.349,589.775 930.346,626.567 931.342,660.837 932.339,696.836 933.335,725.804 934.331,755.215 935.328,706.366 936.324,741.193 937.321,775.489 \n",
       "  938.317,807.931 939.313,845.431 940.31,880.935 941.306,911.515 942.303,943.025 943.299,977.919 944.295,1003.92 945.292,770.411 946.288,801.585 947.285,837.949 \n",
       "  948.281,871.457 949.277,905.724 950.274,937.966 951.27,972.595 952.267,1004.76 953.263,1039.47 954.259,1069.77 955.256,559.412 956.252,507.906 957.249,537.703 \n",
       "  958.245,578.151 959.241,608.026 960.238,647.318 961.234,667.866 962.231,702.124 963.227,732.529 964.223,759.844 965.22,753.815 966.216,736.869 967.213,761.833 \n",
       "  968.209,800.869 969.205,835.99 970.202,862.084 971.198,893.573 972.195,931.745 973.191,956.499 974.187,993.531 975.184,798.619 976.18,592.93 977.177,630.983 \n",
       "  978.173,662.734 979.169,696.161 980.166,735.654 981.162,768.03 982.159,803.149 983.155,837.455 984.151,868.709 985.148,878.173 986.144,859.898 987.141,895.271 \n",
       "  988.137,931.371 989.133,963.211 990.13,997.867 991.126,1038.23 992.123,1073.03 993.119,1106.75 994.115,1146.29 995.112,1031.94 996.108,554.387 997.105,588.569 \n",
       "  998.101,617.741 999.097,656.364 1000.09,688.855 1001.09,723.085 1002.09,755.537 1003.08,784.277 1004.08,819.871 1005.08,789.725 1006.07,218.36 1007.07,264.048 \n",
       "  1008.06,284.037 1009.06,322.859 1010.06,357.737 1011.05,389.622 1012.05,419.859 1013.05,457.802 1014.04,486.797 1015.04,505.923 1016.04,706.345 1017.03,756.044 \n",
       "  1018.03,787.025 1019.03,819.853 1020.02,849.769 1021.02,877.035 1022.01,912.001 1023.01,940.796 1024.01,965.977 1025,999.109 1026,847.397 1027,829.946 \n",
       "  1027.99,866.699 1028.99,889.522 1029.99,932.016 1030.98,953.96 1031.98,991.379 1032.97,1025.62 1033.97,1057.9 1034.97,1086.77 1035.96,870.721 1036.96,745.647 \n",
       "  1037.96,777.596 1038.95,814.442 1039.95,841.665 1040.95,877.755 1041.94,909.998 1042.94,938.872 1043.94,972.216 1044.93,1002.25 1045.93,734.515 1046.92,447.501 \n",
       "  1047.92,493.425 1048.92,512.998 1049.91,564.093 1050.91,582.587 1051.91,618.263 1052.9,639.447 1053.9,676.612 1054.9,710.293 1055.89,734.737 1056.89,687.559 \n",
       "  1057.88,729.223 1058.88,755.473 1059.88,790.571 1060.87,823.049 1061.87,854.678 1062.87,892.701 1063.86,922.802 1064.86,955.341 1065.86,982.779 1066.85,897.985 \n",
       "  1067.85,932.497 1068.85,966.027 1069.84,994.313 1070.84,1033.16 1071.83,1058.64 1072.83,1101.76 1073.83,1132.93 1074.82,1167.37 1075.82,1199.24 1076.82,684.791 \n",
       "  1077.81,715.844 1078.81,740.876 1079.81,774.783 1080.8,807.209 1081.8,840.401 1082.79,873.956 1083.79,905.824 1084.79,935.591 1085.78,972.29 1086.78,689.022 \n",
       "  1087.78,647.811 1088.77,669.255 1089.77,706.319 1090.77,742.899 1091.76,777.211 1092.76,812.771 1093.76,845.068 1094.75,875.028 1095.75,907.321 1096.74,962.006 \n",
       "  1097.74,996.865 1098.74,1024.56 1099.73,1056.13 1100.73,1094.64 1101.73,1123.15 1102.72,1163.23 1103.72,1191.18 1104.72,1220.86 1105.71,1252.88 1106.71,875.026 \n",
       "  1107.7,500.855 1108.7,524.298 1109.7,548.528 1110.69,590.264 1111.69,628.878 1112.69,644.392 1113.68,670.566 1114.68,713.236 1115.68,741.256 1116.67,697.231 \n",
       "  1117.67,572.773 1118.67,603.436 1119.66,628.744 1120.66,664.423 1121.65,696.017 1122.65,726.965 1123.65,753.9 1124.64,790.347 1125.64,823.654 1126.64,807.044 \n",
       "  1127.63,611.146 1128.63,645.141 1129.63,672.212 1130.62,705.832 1131.62,739.098 1132.61,767.827 1133.61,800.778 1134.61,831.7 1135.6,867.453 1136.6,893.624 \n",
       "  1137.6,767.323 1138.59,801.954 1139.59,832.547 1140.59,861.815 1141.58,897.241 1142.58,931.666 1143.58,960.851 1144.57,997.46 1145.57,1028.35 1146.56,1056.14 \n",
       "  1147.56,477.024 1148.56,442.095 1149.55,477.108 1150.55,516.758 1151.55,543.092 1152.54,584.683 1153.54,611.785 1154.54,644.742 1155.53,674.917 1156.53,707.712 \n",
       "  1157.52,754.649 1158.52,776.395 1159.52,810.55 1160.51,841.936 1161.51,876.184 1162.51,904.265 1163.5,938.173 1164.5,971.366 1165.5,1003.11 1166.49,1033.82 \n",
       "  1167.49,944.737 1168.49,890.845 1169.48,925.01 1170.48,962.011 1171.47,994.758 1172.47,1035.64 1173.47,1067.62 1174.46,1101.45 1175.46,1144.42 1176.46,1179.23 \n",
       "  1177.45,927.897 1178.45,545.232 1179.45,589.099 1180.44,623.676 1181.44,654.385 1182.43,686.831 1183.43,720.488 1184.43,756.046 1185.42,786.568 1186.42,817.17 \n",
       "  1187.42,738.72 1188.41,424.143 1189.41,465.764 1190.41,496.907 1191.4,531.701 1192.4,556.324 1193.4,600.72 1194.39,626.994 1195.39,658.523 1196.38,695.947 \n",
       "  1197.38,715.708 1198.38,622.872 1199.37,661.314 1200.37,698.514 1201.37,731.958 1202.36,761.389 1203.36,798.151 1204.36,822.038 1205.35,862.471 1206.35,890.469 \n",
       "  1207.34,926.203 1208.34,864.197 1209.34,891.093 1210.33,927.785 1211.33,950.26 1212.33,984.794 1213.32,1015.55 1214.32,1051.31 1215.32,1080.49 1216.31,1113.2 \n",
       "  1217.31,1135.72 1218.31,681.792 1219.3,571.505 1220.3,608.438 1221.29,647.333 1222.29,669.791 1223.29,707.733 1224.28,741.288 1225.28,769.618 1226.28,804.225 \n",
       "  1227.27,839.336 1228.27,924.112 1229.27,939.948 1230.26,969.643 1231.26,1006.79 1232.25,1037.96 1233.25,1071.97 1234.25,1107.29 1235.24,1137.47 1236.24,1179.31 \n",
       "  1237.24,1204.75 1238.23,1017.21 1239.23,803.659 1240.23,841.986 1241.22,880.742 1242.22,919.619 1243.22,950.67 1244.21,986.422 1245.21,1021.83 1246.2,1060.47 \n",
       "  1247.2,1095.26 1248.2,1046.06 1249.19,851.587 1250.19,886.958 1251.19,921.716 1252.18,955.925 1253.18,992.158 1254.18,1028.99 1255.17,1066.41 1256.17,1100.37 \n",
       "  1257.16,1134.71 1258.16,1143.78 1259.16,1025.96 1260.15,1065.62 1261.15,1099.48 1262.15,1137.48 1263.14,1168.37 1264.14,1198.8 1265.14,1239.84 1266.13,1278.3 \n",
       "  1267.13,1134.67 1268.13,681.442 1269.12,718.785 1270.12,758.299 1271.11,796.286 1272.11,829.967 1273.11,867.048 1274.1,903.529 1275.1,945.159 1276.1,979.165 \n",
       "  1277.09,949.343 1278.09,349.44 1279.09,393.966 1280.08,430.986 1281.08,460.812 1282.07,505.853 1283.07,540.593 1284.07,586.187 1285.06,611.818 1286.06,652.242 \n",
       "  1287.06,688.074 1288.05,625.216 1289.05,658.412 1290.05,689.936 1291.04,725.211 1292.04,760.557 1293.04,798.609 1294.03,834.958 1295.03,867.029 1296.02,899.712 \n",
       "  1297.02,940.638 1298.02,614.578 1299.01,537.161 1300.01,571.02 1301.01,606.306 1302,639.274 1303,674.315 1304,709.985 1304.99,743.411 1305.99,770.341 \n",
       "  1306.98,805.073 1307.98,786.786 1308.98,776.634 1309.97,812.975 1310.97,842.914 1311.97,875.758 1312.96,908.861 1313.96,941.017 1314.96,976.122 1315.95,1009.2 \n",
       "  1316.95,1046.59 1317.95,737.609 1318.94,621.449 1319.94,656.197 1320.93,689.993 1321.93,717.441 1322.93,753.832 1323.92,787.112 1324.92,821.216 1325.92,851.698 \n",
       "  1326.91,883.417 1327.91,856.87 1328.91,822.037 1329.9,858.077 1330.9,892.441 1331.89,922.855 1332.89,954.815 1333.89,995.223 1334.88,1025.1 1335.88,1062.06 \n",
       "  1336.88,1099.42 1337.87,1046.82 1338.87,915.714 1339.87,947.469 1340.86,983.086 1341.86,1015.48 1342.86,1050.51 1343.85,1080.57 1344.85,1117.85 1345.84,1148.14 \n",
       "  1346.84,1180.67 1347.84,1081.02 1348.83,643.88 1349.83,680.428 1350.83,716.629 1351.82,745.301 1352.82,780.449 1353.82,815.118 1354.81,843.13 1355.81,880.105 \n",
       "  1356.8,911.136 1357.8,919.028 1358.8,493.155 1359.79,518.558 1360.79,560.226 1361.79,585.976 1362.78,623.345 1363.78,657.667 1364.78,695.691 1365.77,722.72 \n",
       "  1366.77,763.075 1367.77,791.815 1368.76,835.455 1369.76,860.687 1370.75,892.987 1371.75,921.447 1372.75,955.904 1373.74,993.458 1374.74,1023.81 1375.74,1058.43 \n",
       "  1376.73,1090.35 1377.73,1123.5 1378.73,541.063 1379.72,413.125 1380.72,434.733 1381.71,481.608 1382.71,519.126 1383.71,550.339 1384.7,571.031 1385.7,614.439 \n",
       "  1386.7,647.83 1387.69,675.752 1388.69,766.698 1389.69,814.342 1390.68,852.701 1391.68,885.046 1392.68,918.428 1393.67,957.276 1394.67,995.212 1395.66,1024.63 \n",
       "  1396.66,1066.32 1397.66,1085.21 1398.65,906.923 1399.65,698.025 1400.65,734.112 1401.64,767.784 1402.64,797.832 1403.64,833.827 1404.63,861.688 1405.63,896.993 \n",
       "  1406.62,929.832 1407.62,966.946 1408.62,963.372 1409.61,874.242 1410.61,909.895 1411.61,942.399 1412.6,977.239 1413.6,1015.5 1414.6,1047.68 1415.59,1078.3 \n",
       "  1416.59,1118.59 1417.59,1150.45 1418.58,1103.22 1419.58,568.862 1420.57,603.699 1421.57,634.781 1422.57,678.3 1423.56,706.385 1424.56,744.582 1425.56,776.131 \n",
       "  1426.55,809.174 1427.55,844.16 1428.55,873.684 1429.54,520.454 1430.54,538.333 1431.53,571.381 1432.53,605.926 1433.53,641.347 1434.52,671.484 1435.52,700.065 \n",
       "  1436.52,730.183 1437.51,760.116 1438.51,793.655 1439.51,668.699 1440.5,656.812 1441.5,682.314 1442.5,710.846 1443.49,747.865 1444.49,779.817 1445.48,806.932 \n",
       "  1446.48,837.676 1447.48,866.626 1448.47,897.666 1449.47,734.628 1450.47,669.557 1451.46,706.733 1452.46,737.584 1453.46,772.074 1454.45,804.924 1455.45,842.191 \n",
       "  1456.44,874.703 1457.44,914.464 1458.44,946.787 1459.43,838.819 1460.43,661.227 1461.43,695.229 1462.42,726.573 1463.42,755.605 1464.42,788.414 1465.41,817.544 \n",
       "  1466.41,853.408 1467.41,885.633 1468.4,914.965 1469.4,869.052 1470.39,709.339 1471.39,745.27 1472.39,776.56 1473.38,806.919 1474.38,838.371 1475.38,874.933 \n",
       "  1476.37,901.656 1477.37,936.94 1478.37,968.208 1479.36,836.13 1480.36,250.189 1481.35,289.582 1482.35,332.514 1483.35,371.855 1484.34,384.29 1485.34,406.725 \n",
       "  1486.34,458.932 1487.33,487.089 1488.33,494.791 1489.33,528.533 1490.32,598.477 1491.32,620.349 1492.32,659.026 1493.31,691.215 1494.31,713.594 1495.3,746.802 \n",
       "  1496.3,773.515 1497.3,805.927 1498.29,835.078 1499.29,862.958 1500.29,744.369 1501.28,760.746 1502.28,786.307 1503.28,818.272 1504.27,848.392 1505.27,877.645 \n",
       "  1506.26,914.78 1507.26,940.121 1508.26,970.775 1509.25,1004.62 1510.25,716.299 1511.25,539.954 1512.24,570.231 1513.24,604.073 1514.24,628.487 1515.23,653.622 \n",
       "  1516.23,683.769 1517.23,712.437 1518.22,743.457 1519.22,762.869 1520.21,717.358 1521.21,675.883 1522.21,711.862 1523.2,730.517 1524.2,771.193 1525.2,802.692 \n",
       "  1526.19,827.845 1527.19,852.665 1528.19,887.762 1529.18,913.824 1530.18,893.722 1531.17,844.047 1532.17,875.362 1533.17,900.397 1534.16,930.887 1535.16,963.951 \n",
       "  1536.16,990.241 1537.15,1021.57 1538.15,1053.61 1539.15,1080.2 1540.14,991.556 1541.14,672.383 1542.14,713.541 1543.13,737.584 1544.13,779.451 1545.12,808.995 \n",
       "  1546.12,842.492 1547.12,870.852 1548.11,902.626 1549.11,937.446 1550.11,923.595 1551.1,520.089 1552.1,545.682 1553.1,584.253 1554.09,616.577 1555.09,644.014 \n",
       "  1556.08,669.439 1557.08,707.423 1558.08,736.859 1559.07,771.134 1560.07,796.392 1561.07,698.4 1562.06,716.132 1563.06,748.689 1564.06,774.781 1565.05,809.019 \n",
       "  1566.05,839.657 1567.05,868.094 1568.04,894.546 1569.04,919.38 1570.03,960.204 1571.03,497.266 1572.03,399.419 1573.02,427.592 1574.02,471.747 1575.02,497.983 \n",
       "  1576.01,522.377 1577.01,554.518 1578.01,589.609 1579,615.786 1580,646.043 1580.99,763.733 1581.99,821.754 1582.99,856.643 1583.98,886.184 1584.98,917.798 \n",
       "  1585.98,940.449 1586.97,972.747 1587.97,1003.36 1588.97,1030.1 1589.96,1065.87 1590.96,890.033 1591.96,635.291 1592.95,664.248 1593.95,692.858 1594.94,725.55 \n",
       "  1595.94,755.689 1596.94,782.374 1597.93,806.948 1598.93,841.719 1599.93,869.126 1600.92,901.827 1601.92,890.786 1602.92,917.08 1603.91,946.438 1604.91,978.149 \n",
       "  1605.9,1008.87 1606.9,1037.03 1607.9,1073.83 1608.89,1108.16 1609.89,1136.62 1610.89,1109.06 1611.88,812.291 1612.88,840.445 1613.88,869.75 1614.87,904.283 \n",
       "  1615.87,939.989 1616.87,963.53 1617.86,993.461 1618.86,1022.9 1619.85,1052.19 1620.85,1082.03 1621.85,433.758 1622.84,479.493 1623.84,508.49 1624.84,543.367 \n",
       "  1625.83,570.158 1626.83,604.793 1627.83,627.916 1628.82,657.606 1629.82,688.026 1630.81,723.16 1631.81,821.561 1632.81,852.558 1633.8,880.482 1634.8,915.032 \n",
       "  1635.8,942.799 1636.79,977.275 1637.79,1003.46 1638.79,1033.89 1639.78,1063.69 1640.78,1093.52 1641.78,769.082 1642.77,644.942 1643.77,673.632 1644.76,696.239 \n",
       "  1645.76,731.677 1646.76,757.851 1647.75,786.807 1648.75,816.39 1649.75,844.47 1650.74,877.167 1651.74,739.041 1652.74,586.187 1653.73,614.786 1654.73,644.609 \n",
       "  1655.72,673.627 1656.72,702.593 1657.72,731.404 1658.71,758.204 1659.71,788.189 1660.71,820.275 1661.7,767.125 1662.7,573.475 1663.7,600.102 1664.69,628.679 \n",
       "  1665.69,658.608 1666.69,681.274 1667.68,715.809 1668.68,738.848 1669.67,764.421 1670.67,789.053 1671.67,776.727 1672.66,650.359 1673.66,687.49 1674.66,710.605 \n",
       "  1675.65,734.952 1676.65,768.175 1677.65,801 1678.64,830.731 1679.64,853.021 1680.63,897.001 1681.63,890.232 1682.63,482.363 1683.62,514.069 1684.62,539.249 \n",
       "  1685.62,571.946 1686.61,611.855 1687.61,643.982 1688.61,663.252 1689.6,690.373 1690.6,707.475 1691.6,740.039 1692.59,785.109 1693.59,816.598 1694.58,845.865 \n",
       "  1695.58,873.39 1696.58,900.161 1697.57,931.777 1698.57,960.568 1699.57,985.409 1700.56,1008.5 1701.56,1035.22 1702.56,706.178 1703.55,593.914 1704.55,627.344 \n",
       "  1705.54,659.66 1706.54,686.227 1707.54,721.116 1708.53,744.912 1709.53,774.638 1710.53,798.039 1711.52,831.724 1712.52,727.531 1713.52,674.57 1714.51,702.719 \n",
       "  1715.51,727.049 1716.51,758.065 1717.5,782.158 1718.5,810.992 1719.49,842.116 1720.49,871.199 1721.49,893.191 1722.48,852.335 1723.48,757.422 1724.48,793.995 \n",
       "  1725.47,820.849 1726.47,850.161 1727.47,883.999 1728.46,905.203 1729.46,932.7 1730.45,964.604 1731.45,990.566 1732.45,976.661 1733.44,885.737 1734.44,922.303 \n",
       "  1735.44,947.547 1736.43,985.687 1737.43,1027.42 1738.43,1060.16 1739.42,1090.95 1740.42,1125.47 1741.42,1167.08 1742.41,1100.14 1743.41,465.579 1744.4,494.111 \n",
       "  1745.4,536.007 1746.4,585.467 1747.39,605.692 1748.39,641.846 1749.39,682.305 1750.38,710.418 1751.38,745.542 1752.38,778.065 1753.37,502.851 1754.37,512.988 \n",
       "  1755.36,563.766 1756.36,594.67 1757.36,626.436 1758.35,661.663 1759.35,694.267 1760.35,727.437 1761.34,751.862 1762.34,667.135 1763.34,686.119 1764.33,710.824 \n",
       "  1765.33,740.943 1766.33,765.14 1767.32,799.333 1768.32,829.873 1769.31,858.872 1770.31,887.904 1771.31,915.146 1772.3,875.312 1773.3,878.812 1774.3,907.342 \n",
       "  1775.29,934.443 1776.29,968.304 1777.29,992.835 1778.28,1023.05 1779.28,1050.01 1780.27,1077.3 1781.27,1106.94 1782.27,735.736 1783.26,544.758 1784.26,573.676 \n",
       "  1785.26,617.356 1786.25,650.634 1787.25,677.424 1788.25,711.833 1789.24,748.885 1790.24,768.071 1791.24,807.704 1792.23,721.256 1793.23,658.138 1794.22,687.383 \n",
       "  1795.22,716.791 1796.22,754.498 1797.21,777.368 1798.21,805.333 1799.21,838.652 1800.2,856.613 1801.2,895.23 1802.2,791.025 1803.19,591.323 1804.19,622.728 \n",
       "  1805.18,660.81 1806.18,690.403 1807.18,725.889 1808.17,699.327 1809.17,565.983 1810.17,594.178 1811.16,621.868 1812.16,648.919 1813.16,675.234 1814.15,708.906 \n",
       "  1815.15,737.825 1816.15,764.84 1817.14,793.316 1818.14,800.279 1819.13,702.741 1820.13,729.291 1821.13,760.405 1822.12,787.367 1823.12,817.733 1824.12,839.024 \n",
       "  1825.11,870.643 1826.11,899.397 1827.11,925.513 1828.1,945.49 1829.1,581.597 1830.09,608.25 1831.09,631.215 1832.09,665.703 1833.08,687.133 1834.08,723.364 \n",
       "  1835.08,748.52 1836.07,776.331 1837.07,810.426 1838.07,833.03 1839.06,879.866 1840.06,903.816 1841.06,935.181 1842.05,954.489 1843.05,986.544 1844.04,1019.76 \n",
       "  1845.04,1043.95 1846.04,1081.83 1847.03,1105.08 1848.03,1138.92 1849.03,923.632 1850.02,851.38 1851.02,885.314 1852.02,913.57 1853.01,950.817 1854.01,975.663 \n",
       "  1855,1009.94 1856,1037.85 1857,1068.66 1857.99,1102.12 1858.99,808.493 1859.99,537.272 1860.98,571.936 1861.98,600.851 1862.98,626.618 1863.97,658.933 \n",
       "  1864.97,688.855 1865.96,761.781 1866.96,800.152 1867.96,823.946 1868.95,852.978 1869.95,880.617 1870.95,912.574 1871.94,941.87 1872.94,971.837 1873.94,1003.43 \n",
       "  1874.93,1032.07 1875.93,588.714 1876.93,411.142 1877.92,445.501 1878.92,474.252 1879.91,497.151 1880.91,531.342 1881.91,557.178 1882.9,581.744 1883.9,604.491 \n",
       "  1884.9,641.405 1885.89,530.062 1886.89,436.731 1887.89,464.415 1888.88,494.576 1889.88,525.52 1890.87,544.979 1891.87,579.229 1892.87,609.229 1893.86,632.846 \n",
       "  1894.86,656.538 1895.86,691.472 1896.85,680.893 1897.85,701.673 1898.85,730.001 1899.84,756.988 1900.84,783.56 1901.84,815.89 1902.83,838.852 1903.83,862.828 \n",
       "  1904.82,891.092 1905.82,894.747 1906.82,828.859 1907.81,852.83 1908.81,887.093 1909.81,917.313 1910.8,943.589 1911.8,972.873 1912.8,986.702 1913.79,1020.44 \n",
       "  1914.79,1045.94 1915.78,1056.91 1916.78,750.205 1917.78,790.138 1918.77,820.408 1919.77,838.308 1920.77,865.856 1921.76,897.964 1922.76,921.303 1923.76,957.238 \n",
       "  1924.75,900.66 1925.75,588.469 1926.75,619.624 1927.74,653.063 1928.74,677.295 1929.73,706.039 1930.73,720.621 1931.73,702.745 1932.72,730.853 1933.72,766.463 \n",
       "  1934.72,788.922 1935.71,820.989 1936.71,843.179 1937.71,873.391 1938.7,853.591 1939.7,804.294 1940.69,826.518 1941.69,855.507 1942.69,890.092 1943.68,916.751 \n",
       "  1944.68,651.764 1945.68,415.413 1946.67,445.38 1947.67,458.949 1948.67,492.824 1949.66,521.881 1950.66,534.769 1951.66,573.387 1952.65,594.053 1953.65,626.513 \n",
       "  1954.64,619.201 1955.64,561.211 1956.64,590.355 1957.63,622.509 1958.63,651.137 1959.63,677.143 1960.62,701.941 1961.62,719.298 1962.62,748.917 1963.61,776.831 \n",
       "  1964.61,721.144 1965.6,470.017 1966.6,490.369 1967.6,524.582 1968.59,545.37 1969.59,574.688 1970.59,600.366 1971.58,616.237 1972.58,648.104 1973.58,676.895 \n",
       "  1974.57,659.594 1975.57,512.61 1976.57,531.627 1977.56,558.217 1978.56,572.52 1979.55,598.412 1980.55,621.146 1981.55,659.221 1982.54,676.604 1983.54,694.752 \n",
       "  1984.54,728.159 1985.53,795.229 1986.53,819.664 1987.53,848.455 1988.52,871.265 1989.52,891.996 1990.51,922.529 1991.51,946.463 1992.51,973.04 1993.5,993.682 \n",
       "  1994.5,1016.64 1995.5,574.668 1996.49,531.561 1997.49,541.726 1998.49,567.521 1999.48,586.269 2000.48,615.267 2001.48,638.328 2002.47,672.152 2003.47,697.416 \n",
       "  2004.46,703.65 2005.46,753.507 2006.46,789.754 2007.45,810.903 2008.45,837.073 2009.45,861.18 2010.44,886.689 2011.44,919.994 2012.44,948.712 2013.43,976.164 \n",
       "  2014.43,1003.98 2015.42,770.75 2016.42,590.211 2017.42,618.018 2018.41,667.989 2019.41,685.849 2020.41,718.61 2021.4,807.121 2022.4,834.872 2023.4,858.644 \n",
       "  2024.39,881.899 2025.39,910.71 2026.39,942.635 2027.38,962.384 2028.38,1000.84 2029.37,1020.83 2030.37,1046.86 2031.37,908.854 2032.36,810.409 2033.36,842.953 \n",
       "  2034.36,870.847 2035.35,902.771 2036.35,939.476 2037.35,970.239 2038.34,1003.87 2039.34,1029.99 2040.33,1068.86 2041.33,985.837 2042.33,870.848 2043.32,906.569 \n",
       "  2044.32,942.159 2045.32,979.131 2046.31,1012.39 2047.31,1050.85 2048.31,1091.93 2049.3,1108.94 2050.3,1159.62 2051.3,940.537 2052.29,390.058 2053.29,423.593 \n",
       "  2054.28,459.213 2055.28,506.293 2056.28,554.248 2057.27,592.169 2058.27,625.713 2059.27,650.538 2060.26,685.451 2061.26,677.698 2062.26,575.897 2063.25,593.278 \n",
       "  2064.25,632.526 2065.24,646.511 2066.24,682.217 2067.24,699.036 2068.23,727.566 2069.23,750.257 2070.23,787.725 2071.22,808.453 2072.22,343.544 2073.22,417.688 \n",
       "  2074.21,437.851 2075.21,467.159 2076.21,492.892 2077.2,505.579 2078.2,534.409 2079.19,568.021 2080.19,583.378 2081.19,576.473 2082.18,456.362 2083.18,483.967 \n",
       "  2084.18,499.364 2085.17,523.525 2086.17,556.441 2087.17,564.954 2088.16,595.195 2089.16,612.496 2090.15,639.986 2091.15,661.755 2092.15,708.883 2093.14,734.646 \n",
       "  2094.14,758.823 2095.14,779.754 2096.13,807.974 2097.13,835.094 2098.13,852.618 2099.12,883.343 2100.12,912.487 2101.12,905.565 2102.11,584.431 2103.11,610.485 \n",
       "  2104.1,634.134 2105.1,664.629 2106.1,688.828 2107.09,709.838 2108.09,733.35 2109.09,754.408 2110.08,773.779 2111.08,807.758 2112.08,594.934 2113.07,604.874 \n",
       "  2114.07,621.244 2115.06,646.293 2116.06,665.281 2117.06,694.004 2118.05,715.524 2119.05,741.209 2120.05,762.892 2121.04,789.696 2122.04,806.055 2123.04,828.191 \n",
       "  2124.03,846.449 2125.03,871.13 2126.03,899.491 2127.02,917.078 2128.02,941.494 2129.01,968.325 2130.01,987.321 2131.01,1010.8 2132,857.002 2133,762.18 \n",
       "  2134,802.491 2134.99,816.268 2135.99,849.993 2136.99,873.052 2137.98,899.632 2138.98,930.288 2139.97,956.582 2140.97,983.717 2141.97,881.222 2142.96,721.129 \n",
       "  2143.96,754.228 2144.96,793.444 2145.95,822.843 2146.95,851.251 2147.95,888.386 2148.94,923.066 2149.94,740.049 2150.94,711.774 2151.93,743.67 2152.93,781.644 \n",
       "  2153.92,802.013 2154.92,827.182 2155.92,853.998 2156.91,880.109 2157.91,907.403 2158.91,932.503 2159.9,793.405 2160.9,741.034 2161.9,779.916 2162.89,810.987 \n",
       "  2163.89,849.48 2164.88,879.911 2165.88,921.456 2166.88,909.741 2167.87,809.911 2168.87,842.207 2169.87,878.501 2170.86,909.875 2171.86,939.662 2172.86,974.928 \n",
       "  2173.85,1002.08 2174.85,1031.01 2175.85,1071.14 2176.84,1061.43 2177.84,813.351 2178.83,851.719 2179.83,884.242 2180.83,914.392 2181.82,946.371 2182.82,978.403 \n",
       "  2183.82,1015.38 2184.81,1045.51 2185.81,1086.74 2186.81,1095.09 2187.8,554.614 2188.8,587.823 2189.79,618.357 2190.79,662.014 2191.79,698.7 2192.78,732.278 \n",
       "  2193.78,762.815 2194.78,799.45 2195.77,835.082 2196.77,864.472 2197.77,540.561 2198.76,515.307 2199.76,548.992 2200.76,588.849 2201.75,625.74 2202.75,663.287 \n",
       "  2203.74,694.439 2204.74,731.372 2205.74,764.924 2206.73,704.653 2207.73,716.99 2208.73,748.493 2209.72,784.83 2210.72,819.402 2211.72,853.539 2212.71,889.423 \n",
       "  2213.71,920.046 2214.7,956.716 2215.7,991.032 2216.7,694.133 2217.69,566.867 2218.69,597.847 2219.69,632.25 2220.68,670.461 2221.68,697.101 2222.68,719.907 \n",
       "  2223.67,751.606 2224.67,779.168 2225.67,804.277 2226.66,835.951 2227.66,852.152 2228.65,887.794 2229.65,920.735 2230.65,945.603 2231.64,983.643 2232.64,1016.55 \n",
       "  2233.64,1053.23 2234.63,1083.43 2235.63,1126.78 2236.63,913.014 2237.62,501.86 2238.62,541.03 2239.61,574.637 2240.61,610.632 2241.61,636.582 2242.6,676.911 \n",
       "  2243.6,699.364 2244.6,723.852 2245.59,751.131 2246.59,795.349 2247.59,882.25 2248.58,910.476 2249.58,940.039 2250.58,966.976 2251.57,1008.5 2252.57,1038.83 \n",
       "  2253.56,1073.11 2254.56,1101.68 2255.56,1137.8 2256.55,1135.73 2257.55,609.361 2258.55,643.849 2259.54,674.717 2260.54,713.445 2261.54,743.302 \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip0601)\" points=\"\n",
       "1958.43,251.724 2249.26,251.724 2249.26,130.764 1958.43,130.764 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1958.43,251.724 2249.26,251.724 2249.26,130.764 1958.43,130.764 1958.43,251.724 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0601)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1982.43,191.244 2126.43,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2150.43, 208.744)\" x=\"2150.43\" y=\"208.744\">y1</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "Plots.plot(losses; xlabel=\"Minibatch\", ylabel=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error in running finalizer: ErrorException(\"type TPUSession has no field ptr\")\n",
      "error in running finalizer: ErrorException(\"type TPUSession has no field ptr\")\n",
      "error in running finalizer: ErrorException(\"type TPUSession has no field ptr\")\n"
     ]
    }
   ],
   "source": [
    "# Once it's done, we can fetch the result of the loop task to get back the trained model.\n",
    "# It comes back as an opaque (parameterized) type, so we explicitly convert it to its proper type here.\n",
    "ret = fetch(loop_task)\n",
    "trained_model = convert(typeof(ret).parameters[1], ret);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element XRTArray{Float32,(64,),1}:\n",
       " -8.306219e-6  \n",
       " -8.700178e-6  \n",
       "  4.8199024e-7 \n",
       "  4.9694986e-6 \n",
       " -4.3304617e-6 \n",
       "  2.720569e-7  \n",
       "  1.3269871e-5 \n",
       " -5.145514e-6  \n",
       "  1.2181407e-5 \n",
       "  4.27565e-6   \n",
       " -1.3694021e-5 \n",
       " -3.1454038e-6 \n",
       " -1.9734427e-6 \n",
       "  ⋮            \n",
       "  3.4659809e-6 \n",
       "  5.6800563e-6 \n",
       "  1.2276586e-5 \n",
       "  1.0798353e-5 \n",
       "  5.233467e-7  \n",
       "  9.820984e-8  \n",
       "  2.3268187e-6 \n",
       " -3.8926105e-6 \n",
       "  1.35995815e-5\n",
       "  9.551347e-6  \n",
       " -6.369485e-6  \n",
       " -6.584251e-7  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show one example tensor to prove that we learned something, and that we got a tensor back\n",
    "trained_model.layers[1].bias"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "b5c31cc7a7824008bea4d165ba7b6e38",
   "lastKernelId": "e36073ee-2e0f-4b5d-bfc5-108a1dc52700"
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0-DEV",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
