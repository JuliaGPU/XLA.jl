{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [`XLA.jl`](https://github.com/JuliaTPU/XLA.jl): ResNet on TPUs - Introduction\n",
    "\n",
    "In this notebook, we will step through the fundamental infrastructure necessary to load a [ResNet50](https://arxiv.org/abs/1512.03385) model, JIT it for the [TPU](https://en.wikipedia.org/wiki/Tensor_processing_unit), and feed it with some data in order to get classifications out.  Once you are comfortable with this material, you may wish to move on to [simple model training on the TPU](2_ResNet_Training.ipynb), followed by [distributed TPU training](3_LSTM.ipynb).\n",
    "\n",
    "## Overview of `XLA.jl` workflow\n",
    "\n",
    "We will define a model in plain Julia using the [`Flux.jl`](https://github.com/FluxML/Flux.jl) framework, that will provide the ResNet 50 model computation.  The model definition is contained within the file [`resnet50.jl`](resnet50.jl), however note that in the near future this will instead be sourced from the Metalhead.jl repository of general computer vision models defined in `Flux.jl`/Julia.\n",
    "\n",
    "We will define a simple set of mappings to convert a standard Julia model to be TPU-runnable.  There are a number of restrictions within the current XLA.jl compiler that must be adhered to for compilation to succeed:\n",
    "\n",
    "* All arrays and scalars must be of type `XRTArray` and must have an element type of `Float32`, including literals.  This unfortunately means that code such as `1./x` must be transformed to `XRTArray(1f0)./x`.  We intend to make this an automatic process in the future, but for the time being we manually define the appropriate helper functions such as `softmax()` that use `XRTArray`'s properly within [`model_utils.jl`](model_utils.jl).\n",
    "\n",
    "* All arrays are immutable, meaning that the definition of some layers such as Batch Normalization in `Flux.jl` must be adapted.  We have created a separate implementation of batchnorm called [`TPUBatchNorm`](https://github.com/JuliaTPU/XLA.jl/blob/015256bf97af1dbddc1ab90e325488de88428ef3/examples/resnet/tpu_batch_norm.jl) that we will use here.\n",
    "\n",
    "We will now load in the necessary packages, instantiating an environment local to these notebooks, and construct the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %"
     ]
    }
   ],
   "source": [
    "# Load package versions that are known to work with TPUs, check that Julia version is a known compatible one\n",
    "if Base.GIT_VERSION_INFO.commit != \"95310a080bffd5cc0d5708f3356998ad521303ff\"\n",
    "    @warn(\"Only the very latest Julia version on the `kf/tpu3` branch is supported!\")\n",
    "end\n",
    "\n",
    "import Pkg\n",
    "if haskey(ENV, \"COLAB_GPU\")\n",
    "    # On colab instantiate from the installed version of XLA.jl\n",
    "    Pkg.activate(\"/content/XLA.jl/examples\")\n",
    "else\n",
    "    # In a checked out copy, this notebook is in the examples directory\n",
    "    Pkg.activate(@__DIR__)\n",
    "end\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Initialized ResNet50 model with 25583464 learnable parameters\n"
     ]
    }
   ],
   "source": [
    "# Load in packages and model definition\n",
    "using TensorFlow, XLA, Flux, Printf\n",
    "include(\"resnet50.jl\")\n",
    "\n",
    "model = resnet50();\n",
    "println(\"=> Initialized ResNet50 model with $(sum(prod(size(p)) for p in params(model))) learnable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a `Flux.jl` ResNet50 model.  The next thing we need to do is to alter the model such that it is compileable via `XLA.jl`.  We will do so by defining a set of mapping functions that take advantage of multiple dispatch to recursively walk the model structure, converting normal arrays to `XRTArray`s, coercing scalar values to `Float32`, and converting `BatchNorm` layers to `TPUBatchNorm` objects.  We reproduce the code here to show its simplicity, but note that this code is also exported from `XLA.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Mapped model to TPU-specific construction\n"
     ]
    }
   ],
   "source": [
    "# Convert scalars to single-element XRTArrays with eltype Float32:\n",
    "map_to_tpu(x::Real) = XRTArray(convert(Float32, x))\n",
    "\n",
    "# Convert arrays to XRTArrays with eltype Float32\n",
    "map_to_tpu(x::AbstractArray) = XRTArray(Float32.(x))\n",
    "\n",
    "# Strip off the TrackedArray coating to get at the data underneath\n",
    "map_to_tpu(x::TrackedArray) = map_to_tpu(Flux.data(x))\n",
    "\n",
    "# Turn Chain objects into ImmutableChain objects which store the computation within their type signature\n",
    "map_to_tpu(x::Chain) = ImmutableChain(tuple(map(map_to_tpu, x.layers)...))\n",
    "\n",
    "# Convert BatchNorm layers into TPUBatchNorm layers, passing all children straight through,\n",
    "# except for the \"active\" child, which is not used by the TPUBatchNorm\n",
    "map_to_tpu(x::BatchNorm) = TPUBatchNorm(map(map_to_tpu, Flux.children(x))[1:end-1]...)\n",
    "\n",
    "# For all other objects, just map the children through `map_to_tpu`.\n",
    "map_to_tpu(x) = Flux.mapchildren(map_to_tpu, x)\n",
    "\n",
    "\n",
    "# Convert our model to the TPU-compatible version\n",
    "tpu_model = map_to_tpu(model)\n",
    "println(\"=> Mapped model to TPU-specific construction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "\n",
    "At this point, we are ready to compile the model.  In order to do so, we must first connect to a TPU or `xrt_server` binary running on a host.  We will connect here to a TPU running on a certain port, and assign the special global variable name `sess` to a `Session` object.  Once we have connected to the TPU, we can use the `@tpu_compile` macro to compile our model down to an executable handle which can then be invoked to actually run the computation upon an `x`.\n",
    "\n",
    "Compilation can take quite a while.  On the GCE machine this notebook was run on, the first compilation took over 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to TPU on 10.240.25.2\n",
      "=> Compiled model in 41.7 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-16 22:50:10.329788: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n"
     ]
    }
   ],
   "source": [
    "# Manually initialize if we're not running in colab\n",
    "if !haskey(ENV, \"COLAB_TPU_ADDR\")\n",
    "    tpu_ip = \"10.240.48.2\"\n",
    "    println(\"Connecting to TPU on $(tpu_ip)\")\n",
    "    XLA.initialize!(\"$(tpu_ip):8470\"; reset=true)\n",
    "end\n",
    "\n",
    "# Get our global XLA/TF session\n",
    "sess = XLA.global_session()\n",
    "\n",
    "# Generate random input tensor; a batch of two images with spatial dimensions 224x224 and 3 color channels.\n",
    "x = randn(Float32, 224, 224, 3, 2)\n",
    "\n",
    "# Compile the model\n",
    "t_start = time()\n",
    "compilation_handle = @tpu_compile tpu_model(XRTArray(x));\n",
    "t_end = time()\n",
    "\n",
    "println(@sprintf(\"=> Compiled model in %.1f seconds\", t_end - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it is compiled, we can run it using `TensorFlow.jl`'s `run()` method.  We must pass first the compilation handle, then a structure containing the weights of the model, then the tensor to be pushed through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×2 Array{Float32,2}:\n",
       "  9.3714e-6     1.5487e-5  \n",
       " -5.82612e-5   -6.02037e-5 \n",
       "  3.61051e-5    3.52789e-5 \n",
       "  7.43437e-5    7.73885e-5 \n",
       " -0.00012566   -0.000127063\n",
       "  4.27789e-6    2.56461e-6 \n",
       " -1.39207e-5   -1.09112e-5 \n",
       " -2.7817e-5    -2.8237e-5  \n",
       "  4.25479e-5    4.3822e-5  \n",
       "  0.000122415   0.00012554 \n",
       "  1.02747e-6    4.76554e-6 \n",
       " -8.77394e-5   -8.47399e-5 \n",
       "  5.84321e-5    5.94616e-5 \n",
       "  ⋮                        \n",
       "  1.13424e-5    1.35719e-5 \n",
       "  3.83681e-5    3.84252e-5 \n",
       " -5.69827e-5   -5.71565e-5 \n",
       " -5.30935e-5   -4.89687e-5 \n",
       " -2.63157e-6   -5.10083e-6 \n",
       "  8.05895e-5    7.91957e-5 \n",
       "  9.4602e-6     9.40557e-6 \n",
       " -7.02242e-6   -5.49835e-6 \n",
       " -4.08621e-5   -3.82836e-5 \n",
       " -5.4052e-5    -5.93312e-5 \n",
       "  8.26676e-5    8.19929e-5 \n",
       " -8.50972e-5   -8.62632e-5 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the actual computation\n",
    "y_hat = run(compilation_handle,\n",
    "    # Transfer model weights\n",
    "    XRTRemoteStruct(sess, tpu_model),\n",
    "    # Transfer `x`\n",
    "    XRTArray(sess, x)\n",
    ")\n",
    "\n",
    "# Convert the output (which is an XRTArray) back to a normal array:\n",
    "y_hat = convert(Array, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, you have successfully translated, compiled, and run your first model on the TPU.  Congratulations!  You should feel very proud of yourself.  Next up, we will [learn to do some training](2_ResNet_Training.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "f7c8cd65af16462c80eb6a19fd047fb7",
   "lastKernelId": "75c6e187-9a05-4b55-bbcd-448a3818a29f"
  },
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "1_ResNet_Intro.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
