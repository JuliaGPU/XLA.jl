{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [`XLA.jl`](https://github.com/JuliaTPU/XLA.jl): Shakespeare LSTM\n",
    "\n",
    "In this notebook, we will showcase using `XLA.jl` with LSTMs to learn the structure of Shakespearean english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %7 %             ]  65.7 % [========================================>]  98.6 %"
     ]
    }
   ],
   "source": [
    "## Load package versions that are known to work with TPUs, check that Julia version is a known compatible one\n",
    "if Base.GIT_VERSION_INFO.commit != \"0424938442a907a35089254d2bd14b731c2008ec\"\n",
    "    @warn(\"Only the very latest Julia version on the `kf/tpu3` branch is supported!\")\n",
    "end\n",
    "\n",
    "import Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/sabae/.julia/compiled/v1.1/TensorFlow/IhIhf.ji for TensorFlow [1d978283-2c37-5f34-9a8e-e9c0ece82495]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Warning: Loading a new version of TensorFlow.jl for the first time. This initial load can take around 5 minutes as code is precompiled; subsequent usage will only take a few seconds.\n",
      "└ @ TensorFlow ~/.julia/packages/TensorFlow/eu9qM/src/TensorFlow.jl:3\n",
      "┌ Info: Recompiling stale cache file /home/sabae/.julia/compiled/v1.1/XLA/bZBiw.ji for XLA [1ae4bca4-de81-11e8-0eca-6d3e4e7c4181]\n",
      "└ @ Base loading.jl:1184\n",
      "WARNING: could not import xla.Shape into XLA\n",
      "WARNING: could not import NNlib.cudata into Tracker\n",
      "┌ Warning: Package XLA does not have Random in its dependencies:\n",
      "│ - If you have XLA checked out for development and have\n",
      "│   added Random as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with XLA\n",
      "└ Loading Random into XLA from project dependency, future warnings for XLA are suppressed.\n",
      "WARNING: Method definition softmax(XLA.XRTArray{T, Dims, N} where N where Dims where T) in module XLA at /home/sabae/.julia/dev/XLA/src/linalg.jl:404 overwritten at /home/sabae/.julia/dev/XLA/src/utils.jl:127.\n",
      "WARNING: Method definition ∇softmax(Any, XLA.XRTArray{T, Dims, N} where N where Dims where T) in module XLA at /home/sabae/.julia/dev/XLA/src/linalg.jl:408 overwritten at /home/sabae/.julia/dev/XLA/src/utils.jl:132.\n",
      "┌ Info: Recompiling stale cache file /home/sabae/.julia/compiled/v1.1/Unrolled/BnVLg.ji for Unrolled [9602ed7d-8fef-5bc8-8597-8f21381861e8]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " => Loaded 4573338-character dataset and encoded into 68-symbol embedding\n"
     ]
    }
   ],
   "source": [
    "using TensorFlow, XLA, Flux, Unrolled, Zygote, Printf, Statistics\n",
    "include(\"tpu_optimizers.jl\")\n",
    "\n",
    "# First, let's download our dataset;\n",
    "if !isfile(\"shakespeare_input.txt\")\n",
    "    download(\"https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\", \"shakespeare_input.txt\")\n",
    "end\n",
    "\n",
    "# Read text in as a giant string, convert to array of characters\n",
    "text = collect(String(read(\"shakespeare_input.txt\")))\n",
    "\n",
    "# Generate alphabet, which we will use as an embedding (along with special \"stop\" character '_')\n",
    "alphabet = sort([unique(text)..., '_'])\n",
    "stop = UInt32(Flux.onehotidx('_', alphabet))\n",
    "\n",
    "# Embed text through alphabet as UInt32 onehot indices\n",
    "text = UInt32.(map(ch -> Flux.onehotidx(ch, alphabet), text))\n",
    "\n",
    "println(\" => Loaded $(length(text))-character dataset and encoded into $(length(alphabet))-symbol embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " => Segmented into 1430 batches of size 64 with 50-element sequences\n"
     ]
    }
   ],
   "source": [
    "# We will process 64 sequences of length 50 at a time.  Reshape `text` into\n",
    "# tensors of shape (seq_len, batch_size, batch_idx).  To reshape cleanly, we\n",
    "# will pad our text with our `stop` character until it is easily reshapable:\n",
    "batch_size = 64\n",
    "seq_len = 50\n",
    "num_batches = ceil(Int, (length(text) - 1)/(seq_len*batch_size))\n",
    "padded_length = seq_len*batch_size*num_batches + 1\n",
    "text = vcat(text, repeat([stop], padded_length - length(text)))\n",
    "\n",
    "# Build Xs and Ys from this text, where each element of `Xs` has its next element\n",
    "# predicted by the corresponding element of `Ys`.\n",
    "Xs = reshape(text[1:end-1], (seq_len, batch_size, num_batches))\n",
    "Ys = reshape(text[2:end-0], (seq_len, batch_size, num_batches))\n",
    "\n",
    "println(\" => Segmented into $(num_batches) batches of size $(batch_size) with $(seq_len)-element sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the LSTM state vectors from a model\n",
    "get_model_state(m::Flux.LSTMCell) = Flux.hidden(m)\n",
    "get_model_state(m::Flux.Recur) = get_model_state(m.cell)\n",
    "function get_model_state(model)\n",
    "    return tuple(\n",
    "        get_model_state(model.layers[1]),\n",
    "        get_model_state(model.layers[2]),\n",
    "    )\n",
    "end\n",
    "\n",
    "# Update LSTM state vectors within a model\n",
    "set_model_state(m::Flux.LSTMCell, state) = Flux.LSTMCell(m.Wi, m.Wh, m.b, state...)\n",
    "set_model_state(m::Flux.Recur, state) = Flux.Recur(set_model_state(m.cell, state))\n",
    "function set_model_state(model, state)\n",
    "    return typeof(model)(\n",
    "        set_model_state(model.layers[1], state[1]),\n",
    "        set_model_state(model.layers[2], state[2]),\n",
    "        model.layers[3],\n",
    "    )\n",
    "end\n",
    "\n",
    "function initialize_state(model, x)\n",
    "    # Run the given x values through the model\n",
    "    h1, h2 = get_model_state(model)\n",
    "    \n",
    "    # Create zero-vectors of the same length (this disregards batch dimension)\n",
    "    zerovec(h) = Zygote.map(sub_h -> zero(sub_h[:,1]), h)\n",
    "    h1, h2 = zerovec.((h1, h2))\n",
    "\n",
    "    # Next, run the new x through the cells to broadcast up the dimensions of h1/h2\n",
    "    h1, x = model.layers[1].cell(h1, x)\n",
    "    h2, x = model.layers[2].cell(h2, x)\n",
    "\n",
    "    # Set the model state and return the model\n",
    "    return set_model_state(model, (h1, h2))\n",
    "end\n",
    "\n",
    "\n",
    "model = Chain(\n",
    "    LSTM(length(alphabet), 128),\n",
    "    LSTM(128, 128),\n",
    "    Dense(128, length(alphabet))\n",
    ")\n",
    "\n",
    "model = initialize_state(model, zeros(Float32, length(alphabet), batch_size))\n",
    "tpu_model = map_to_tpu(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_lstm_unrolled_expansion_ (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function single_lstm_run(model, state, x)\n",
    "    # Unpack model into separate layers\n",
    "    lstm1, lstm2, dense = model.layers\n",
    "\n",
    "    # Unpack state for our LSTM layers\n",
    "    h1, h2 = state\n",
    "    \n",
    "    # Push `x` through, updating our state\n",
    "    h1, x = lstm1(h1, x)\n",
    "    h2, x = lstm2(h2, x)\n",
    "    y_hat = dense(x)\n",
    "\n",
    "    # Return y_hat and our state\n",
    "    return y_hat, (h1, h2)\n",
    "end\n",
    "\n",
    "\n",
    "# Helper function to convert a batch of text at a particular time point into first a OneHotMatrix,\n",
    "# and then densifying that OneHotMatrix into a typical XRTArray{Float32} which we can apply\n",
    "# logitcrossentropy loss upon.\n",
    "function densify(::Val{alphabet_size}, x::XRTArray, t) where {alphabet_size}\n",
    "    return convert(XRTArray{Float32}, Flux.OneHotMatrix(alphabet_size, x[XRTArray(t), :]))\n",
    "end\n",
    "function densify(::Val{alphabet_size}, x, t) where {alphabet_size}\n",
    "    return Flux.OneHotMatrix(alphabet_size, x[t, :])\n",
    "end\n",
    "\n",
    "# This function runs the full lstm.  It's not very easy to return a concatenated `y`\n",
    "# because all XLA.jl code is immutable, so we can't do e.g. y[i] = ...\n",
    "# Luckily, for training, we don't have to, we just accumulate into `loss`.\n",
    "@unroll function full_lstm(unused::Val{alphabet_size}, model, x_batch::XRTArray, y_batch::XRTArray) where {alphabet_size}\n",
    "    # Get current LSTM state\n",
    "    state = get_model_state(model)\n",
    "    \n",
    "    # Accumulate loss into here\n",
    "    loss = XRTArray(0f0)\n",
    "\n",
    "    # Iterate over time\n",
    "    @unroll for time_idx = 1:size(x_batch, 1)\n",
    "        # Create dense representations of the one-hot encoded text at this point in time, across an entire batch\n",
    "        x = densify(Val(alphabet_size), x_batch, time_idx)\n",
    "        \n",
    "        # Push x through our model to get y_hat (and new recurrent state values)\n",
    "        y_hat, state = single_lstm_run(model, state, x)\n",
    "        \n",
    "        # Accumulate loss\n",
    "        loss += crossentropy(softmax(y_hat), densify(Val(alphabet_size), y_batch, time_idx))\n",
    "    end\n",
    "    \n",
    "    model = set_model_state(model, state)\n",
    "    \n",
    "    # Return loss and updated model\n",
    "    return loss, model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_lstm (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_lstm(::Val{alphabet_size}, ::Val{num_epochs}, model, Xs, Ys, η) where {alphabet_size, num_epochs}\n",
    "    # Create optimizer\n",
    "    opt = TPU_ADAM(model, η, (XRTArray(0.9f0), XRTArray(0.999f0)))\n",
    "    \n",
    "    # We will report loss once every epoch, store it here in the meantime:\n",
    "    loss_buffer = zero(XRTArray{Float32, (size(Xs, 3),), 1})\n",
    "\n",
    "    # Iterate over epochs\n",
    "    epoch_idx = XRTArray(1)\n",
    "    while epoch_idx <= XRTArray(num_epochs)\n",
    "        # Iterate over batches within a single epoch\n",
    "        batch_idx = XRTArray(1)\n",
    "        \n",
    "        batch_permutation = XLA.shuffle(XRTArray(1:size(Xs, 3)))\n",
    "        while batch_idx <= XRTArray(size(Xs, 3))\n",
    "            # Calculate forward pass of model, and compile backward pass stored in `back()`.\n",
    "            # Use `let` block to work around Julia inference limitations\n",
    "            (loss, model), back = let model=model,\n",
    "                             x_batch=Xs[:, :, batch_permutation[batch_idx]],\n",
    "                             y_batch=Ys[:, :, batch_permutation[batch_idx]]\n",
    "                Zygote._forward(\n",
    "                    Zygote.Context{Nothing}(nothing),\n",
    "                    model -> full_lstm(Val(alphabet_size), model, x_batch, y_batch),\n",
    "                    model,\n",
    "                )\n",
    "            end\n",
    "            \n",
    "            # Invoke `back()` with sensitivity `1f0` on the `loss`\n",
    "            Δ_model = Zygote.tailmemaybe(back(1f0))[1]\n",
    "\n",
    "            # Cross-replica sum our model updates to average across all tpus\n",
    "            Δ_model = XLA.unflatten_tuple(Δ_model,\n",
    "               XLA.HloCrossReplicaSum{typeof(+)}((), 0, \"\")(\n",
    "                   +,\n",
    "                   XLA.flatten_tuple(Δ_model)...\n",
    "               )\n",
    "            )\n",
    "\n",
    "            # Update parameters via our optimizer\n",
    "            opt, model = update!(opt, model, Δ_model)\n",
    "\n",
    "            # Store loss over an epoch into `loss_buffer`.\n",
    "            loss_buffer = Base.setindex(loss_buffer, loss, batch_idx)\n",
    "\n",
    "            # Increment batch_idx\n",
    "            batch_idx += XRTArray(1)\n",
    "        end\n",
    "        \n",
    "        # Once per epoch, output our training loss for the entire epoch\n",
    "        XLA.HloOutfeed()((loss_buffer,), XLA.HloAfterAll()())\n",
    "        \n",
    "        # Increment epoch_idx\n",
    "        epoch_idx += XRTArray(1)\n",
    "    end\n",
    "    \n",
    "    # Return the trained model (note that this gets returned from each of our TPUs, but we\n",
    "    # only pay attention to the model returned from the first node, since they are all\n",
    "    # identical thanks to the cross-replica sum above in the training loop)\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to TPU on 10.240.25.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-25 15:56:25.790070: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
      "┌ Warning: Compilation failed; attempting to explain suboptimal inference:\n",
      "└ @ Main /home/sabae/.julia/dev/XLA/src/compiler_interface.jl:117\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: is_header && (bb_to_outline == bbs_to_outline[end] && idx == block.stmts[end])",
     "output_type": "error",
     "traceback": [
      "AssertionError: is_header && (bb_to_outline == bbs_to_outline[end] && idx == block.stmts[end])",
      "",
      "Stacktrace:",
      " [1] outline_loop_blocks(::Core.Compiler.IRCode, ::Core.Compiler.DomTree, ::Array{Int64,1}, ::Array{Any,1}, ::Array{Pair{Core.SSAValue,Core.PhiNode},1}, ::Array{Union{Argument, SSAValue},1}, ::Bool) at /home/sabae/.julia/dev/XLA/src/outlining.jl:71",
      " [2] outline_loop_blocks at /home/sabae/.julia/dev/XLA/src/outlining.jl:56 [inlined]",
      " [3] outline_loop_body(::Core.Compiler.IRCode, ::Core.Compiler.DomTree, ::Array{Int64,1}, ::Array{Any,1}, ::Array{Pair{Core.SSAValue,Core.PhiNode},1}, ::Array{Union{Argument, SSAValue},1}) at /home/sabae/.julia/dev/XLA/src/outlining.jl:123",
      " [4] outline_loop!(::Core.Compiler.IRCode, ::Core.Compiler.OptimizationState, ::Core.Compiler.DomTree, ::Int64) at /home/sabae/.julia/dev/XLA/src/outlining.jl:338",
      " [5] outline_control_flow!(::Core.Compiler.IRCode, ::Core.Compiler.OptimizationState) at /home/sabae/.julia/dev/XLA/src/outlining.jl:501",
      " [6] _compile_to_xla!(::Array{XLA.xla.HloComputationProto,1}, ::XLA.xla.HloComputationProto, ::Core.Compiler.IRCode, ::Core.Compiler.OptimizationState) at /home/sabae/.julia/dev/XLA/src/compiler.jl:57",
      " [7] #compile_to_xla#227(::Array{XLA.MeshIndex,1}, ::Function, ::Core.Compiler.IRCode, ::Core.Compiler.OptimizationState) at /home/sabae/.julia/dev/XLA/src/compiler.jl:342",
      " [8] (::getfield(XLA, Symbol(\"#kw##compile_to_xla\")))(::NamedTuple{(:replica_device_coords,),Tuple{Array{XLA.MeshIndex,1}}}, ::typeof(XLA.compile_to_xla), ::Core.Compiler.IRCode, ::Core.Compiler.OptimizationState) at ./none:0",
      " [9] top-level scope at /home/sabae/.julia/dev/XLA/src/compiler_interface.jl:115",
      " [10] top-level scope at In[7]:14"
     ]
    }
   ],
   "source": [
    "tpu_ip = \"10.240.25.3\"\n",
    "println(\"Connecting to TPU on $(tpu_ip)\")\n",
    "\n",
    "# NOTE: If you are connecting to an actual TPU, use `TPUSession`.  If you are\n",
    "# connecting to an `xrt_server`, use `Session()`.\n",
    "sess = TPUSession(\"$(tpu_ip):8470\")\n",
    "\n",
    "num_epochs = 10\n",
    "η = 0.001f0\n",
    "\n",
    "# Compile the model\n",
    "t_start = time()\n",
    "all_tpus = all_tpu_devices(sess)\n",
    "compilation_handle = @tpu_compile devices=all_tpus train_lstm(Val(length(alphabet)), Val(num_epochs), tpu_model, XRTArray(Xs), XRTArray(Ys), XRTArray(0.01f0));\n",
    "t_end = time()\n",
    "\n",
    "println(@sprintf(\"=> Compiled training loop in %.1f seconds\", t_end - t_start))\n",
    "\n",
    "t_start = time()\n",
    "loop_task = XLA.run_on_devices(compilation_handle, tpu_model, Xs, Ys, η)\n",
    "t_end = time()\n",
    "\n",
    "println(@sprintf(\"=> Launched training loop on %d TPUs in %.1f seconds\", length(all_tpus), t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: loop_task not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: loop_task not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[8]:1"
     ]
    }
   ],
   "source": [
    "loop_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an outfeed ops\n",
    "outfeed_ops = [XLA.make_outfeed_on(sess,\n",
    "    # On this device\n",
    "    tpu_device,\n",
    "    \n",
    "    # Which will output this type\n",
    "    Tuple{XRTArray{Float32, (num_batches,), 1},}\n",
    ") for tpu_device in all_tpu_devices(sess)]\n",
    "\n",
    "losses = Float64[]\n",
    "for epoch_idx in 1:num_epochs\n",
    "    # Get loss from TPU 1\n",
    "    epoch_loss = mean(run(sess, outfeed_ops))\n",
    "    append!(losses, epoch_loss)\n",
    "\n",
    "    # Print it out as we go, showing the average loss to (hopefully) watch it decrease\n",
    "    println(\"[$epoch_idx] epoch avg. loss: $(mean(epoch_loss))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "l_idxs = collect(1:length(losses))./num_batches\n",
    "Plots.plot(l_idxs, losses; xlabel=\"Epochs\", ylabel=\"Loss\", legend=nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = fetch(loop_task)\n",
    "trained_model = convert(typeof(ret[1]).parameters[1], ret[1]);\n",
    "\n",
    "# Convert all XRTArray values to just normal arrays:\n",
    "trained_model = map_to_cpu(trained_model)\n",
    "\n",
    "# Resize the internal state vectors to deal with a single batch at a time\n",
    "#trained_model = initialize_state(trained_model, Flux.onehot('a', alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.layers[3]\n",
    "#z.Wi .- convert(Array, trained_model.layers[2].Wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[alphabet[x] for x in Xs[:, 2, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.reset!(trained_model)\n",
    "x = rand(alphabet)\n",
    "print(x)\n",
    "for idx in 1:200\n",
    "    y_hat = softmax(trained_model(Flux.onehot(x, alphabet)))\n",
    "    x = alphabet[argmax(y_hat[1,:])]\n",
    "    print(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".Csvuvt-!J!ibwf!mjlf!mjlf!ijt dpmpvsu-!xfmm!nfu;!gps!epjoh!Dsfttje!xjuipvu!evuz- vomfbnofe!uif!dpvsbhf!pg!njof?!pof!dbmmt!cz!xiptf!dpvousz!ibvout Nblftu!obnf!uif!ljohepn!mjlf!ijt!mpwfmz!upohvf3  JSPT!\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using StatsBase\n",
    "\n",
    "function sample(m, alphabet, len; temp = 1)\n",
    "  m = cpu(m)\n",
    "  Flux.reset!(m)\n",
    "  buf = IOBuffer()\n",
    "  c = rand(alphabet)\n",
    "  for i = 1:len\n",
    "    write(buf, c)\n",
    "    c = wsample(alphabet, softmax(m(Flux.onehot(c, alphabet))))\n",
    "  end\n",
    "  return String(take!(buf))\n",
    "end\n",
    "\n",
    "sample(trained_model, alphabet, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592.3437714576721"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet_size = length(alphabet)\n",
    "trained_model = initialize_state(trained_model, randn(Float32, alphabet_size, 64))\n",
    "x_batch = Xs[:, :, 1]\n",
    "y_batch = Ys[:, :, 1]\n",
    "\n",
    "# Accumulate loss into here\n",
    "loss = 0.0\n",
    "\n",
    "Flux.reset!(trained_model)\n",
    "\n",
    "# Iterate over time\n",
    "for time_idx = 1:size(x_batch, 1)\n",
    "    # Create dense representations of the one-hot encoded text at this point in time, across an entire batch\n",
    "    x = Float32.(densify(Val(alphabet_size), Int64.(x_batch), time_idx))\n",
    "    y = Float32.(densify(Val(alphabet_size), Int64.(y_batch), time_idx))\n",
    "\n",
    "    # Push x through our model to get y_hat (and new recurrent state values)\n",
    "    #y_hat, state = single_lstm_run(trained_model, state, x)\n",
    "    y_hat = trained_model(x)\n",
    "\n",
    "    # Accumulate loss\n",
    "    loss += Flux.logitcrossentropy(y_hat, y)\n",
    "end\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68-element Array{Char,1}:\n",
       " '\\n'\n",
       " ' ' \n",
       " '!' \n",
       " '$' \n",
       " '&' \n",
       " '\\''\n",
       " ',' \n",
       " '-' \n",
       " '.' \n",
       " '3' \n",
       " ':' \n",
       " ';' \n",
       " '?' \n",
       " ⋮   \n",
       " 'o' \n",
       " 'p' \n",
       " 'q' \n",
       " 'r' \n",
       " 's' \n",
       " 't' \n",
       " 'u' \n",
       " 'v' \n",
       " 'w' \n",
       " 'x' \n",
       " 'y' \n",
       " 'z' "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"ir.txt\", \"w\") do io\n",
    "    Base.IRShow.show_ir(io, XLA.code_typed_xla(Tuple{typeof(train_lstm), typeof(Val(length(alphabet))), typeof(Val(num_epochs)), typeof(tpu_model), typeof(XRTArray(Xs)), typeof(XRTArray(Ys)), typeof(XRTArray(0.01f0))})[1]; verbose_linetable=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "181549ff50034a008c9b24c289c34abe",
   "lastKernelId": "21aa7ed2-7d39-466d-be9c-3c6f69ac90cd"
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0-DEV",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
